<br><br><br><br><br><br>

<h1  align=center>ã€Šä¸“ä¸šæ–¹å‘è¯¾ç¨‹è®¾è®¡ã€‹å¤§ä½œä¸š</h1>



<br><br><br><br><br>

<h3  align=center>é¢˜ç›®ï¼šå¤§è§„æ¨¡ç”¨æˆ·è¡Œä¸ºåˆ†æç³»ç»Ÿ</h3>



<br><br><br><br>



| å§“å  | å­¦å·  | ç­çº§  | æˆç»©  |
| ----- | ----- | ----- | ----- |
| <br>  | <br/> | <br/> | <br/> |
| <br/> | <br/> | <br/> | <br/> |
| <br/> | <br/> | <br/> | <br/> |

 

<br><br><br><br>



 



<h5  align=center>å¤§è¿ç†å·¥å¤§å­¦è½¯ä»¶å­¦é™¢ç»Ÿ</h5>

<h5  align=center>2019å¹´7æœˆ</h5>

## é¡¹ç›®è¯´æ˜

ç¥ç­–é¢˜ç›®ï¼šâ€œç¥ç­–åˆ†æâ€ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æäº§å“ï¼ŒåŒ…å«ä¸€ä¸ªå®Œæ•´çš„æ•°æ®ä»“åº“ã€‚æ•°æ®ä»“åº“çš„å»ºè®¾æ˜¯æ•°æ®è¿›ä¸€æ­¥åº”ç”¨çš„åŸºç¡€ã€‚è€Œä¸€ä¸ªå®Œæ•´çš„æ•°æ®ä»“åº“é€šå¸¸æœ‰å¦‚ä¸‹æ¨¡å—ï¼šæ•°æ®é‡‡é›†ï¼ˆSDKã€å¯¼å…¥å·¥å…·ã€LogAgent ç­‰ï¼‰ã€æ•°æ®å¯¼å…¥ï¼ˆæ¸…æ´—ã€å…¥åº“ï¼‰ã€å­˜å‚¨ã€æŸ¥è¯¢å¼•æ“ã€åˆ†ææ¨¡å‹æŠ½è±¡å±‚ã€æ¥å£å±‚ã€UI äº¤äº’å±‚ã€‚å…¶ä¸­ä» â€æ•°æ®é‡‡é›†â€œ åˆ° â€å¯¼å…¥å­˜å‚¨â€œï¼Œæ¶‰åŠçš„æŠ€æœ¯ç»†èŠ‚éå¸¸ç¹æ‚ï¼Œå´åˆä¸èƒ½ç›´è§‚åœ°å›ç­”åˆå­¦è€…çš„ç–‘é—®ï¼šâ€œå…·ä½“ä¸šåŠ¡åœºæ™¯ä¸‹ï¼Œæ•°æ®åˆ†æåˆ°åº•æ˜¯å¦‚ä½•åº”ç”¨çš„â€œ è¿™ä¸ªé—®é¢˜ã€‚ä½†æŸ¥è¯¢å±‚ï¼ˆåŒ…æ‹¬æ¨¡å‹æŠ½è±¡ã€æŸ¥è¯¢å¼•æ“ï¼‰åˆ™ä¸åŒï¼ŒæŸ¥è¯¢ä¸€ä¾§æ˜¯è´´è¿‘ä¸šåŠ¡åº”ç”¨çš„ä¸€ä¾§ï¼Œç›¸å¯¹è€Œè¨€ä¼šæ›´åŠ ç›´è§‚ã€‚æ‰€ä»¥æœ¬æ¬¡çš„é¡¹ç›®æˆ‘ä»¬å°±ä»¥æŸ¥è¯¢å±‚ä¸ºåˆ‡å…¥ç‚¹ï¼Œå»å°è¯•æ­å»ºä¸€ä¸ªå¤§è§„æ¨¡ç”¨æˆ·è¡Œä¸ºåˆ†æç³»ç»Ÿã€‚

## æˆå‘˜åˆ†å·¥

| æˆå‘˜   | å­¦å·      | åˆ†å·¥                                                         |
| ------ | --------- | ------------------------------------------------------------ |
| å¾è¯—ç‘¶ | 201692126 | ä¸šåŠ¡é€»è¾‘è®¾è®¡+Impalaå®ç°+æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ+æ€»ä½“è®¾è®¡+åŠŸèƒ½å®ç°+ä¼˜åŒ–æ–¹æ³•å®ç° |
| ç‹è´   | 201672048 | webåå°æ­å»º+ä¸šåŠ¡é€»è¾‘è®¾è®¡+ç•Œé¢è®¾è®¡+ç•Œé¢å®ç°+ç¯å¢ƒæ­å»º(Haoop+Hive+Impala)+æ•°æ®å¯¼å…¥+ä¼˜åŒ–æ–¹æ³• |
| å´ä»»é«˜ | 201671905 | æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ+æ€§èƒ½æµ‹è¯•+æ­£ç¡®æ€§æµ‹è¯•+ç¼–å†™ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ            |

## ç¯å¢ƒå®‰è£…

### 1.Hadoop 

##### ç¯å¢ƒå‡†å¤‡

æœ¬æœºmacOS Mojave 10.14.1 å°è¯•åœ¨æœ¬åœ°æ­å»ºä¼ªåˆ†å¸ƒå¼Hadoop

##### jdkä¸‹è½½

åˆ°å®˜ç½‘ä¸‹è½½äº†jdk8 jdk-8u191-macosx-x64.dmgå®‰è£…jdk ä¹‹åé…ç½®ç¯å¢ƒå˜é‡å¦‚ä¸‹ï¼š
```
JAVA_HOME="Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home"
export JAVA_HOME
CLASS_PATH="$JAVA_HOME/lib"
PATH=".$PATH:$JAVA_HOME/bin"
export PATH="$HOME/.yarn/bin:$PATH"
```

> æ ¹æ®[è¿™ä¸ªæ•™ç¨‹](https://zhuanlan.zhihu.com/p/31162356)è£…å¥½äº†java

##### sshé…ç½®

å…ˆæŠŠç³»ç»Ÿåå¥½è®¾ç½®-å…±äº«-è¿œç¨‹ç™»å½•æ‰“å¼€
```
ssh localhost
```
æ˜¾ç¤ºéœ€è¦å¯†ç ï¼Œå®é™…ä¸Šå°±æ˜¯æœ¬æœºå¯†ç ï¼Œè¿™æ ·ä¸æ˜¯å¾ˆokï¼ˆå…·ä½“åˆ°åº•å“ªé‡Œä¸okæˆ‘ä¹Ÿä¸æ˜¯å¾ˆæ¸…æ¥š

terminalä¸­ä¿®æ”¹sshè®¾ç½®
```
ssh-keygen -t rsa
[è¿™é‡Œæœ‰å•¥è¾“å…¥çš„ä¸œè¥¿åæ­£æˆ‘ä»¬å°±æŒ‰å›è½¦å°±å®Œäº‹]
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod og-wx ~/.ssh.authorized_keys
```
è¿™æ—¶å€™æˆ‘ä»¬å†æ‰§è¡Œ
```
ssh localhost
```
å°±ä¼šå‘ç°ä¸éœ€è¦å¯†ç sshç™»é™†äº†ï½å°±å¯ä»¥ä¸‹è½½Hadoopäº†å‘¢ï¼

##### Hadoopä¸‹è½½å®‰è£…

###### å®˜ç½‘ä¸‹è½½

[å®˜ç½‘æä¾›çš„ä¸‹è½½åœ°å€](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.8.5/hadoop-2.8.5.tar.gz)æˆ‘ä¸‹è½½äº†2.8.5

ä¸‹è½½å®Œä¹‹åæˆ‘æŠŠè¿™ä¸ªtar.gzæ”¾åˆ°äº†/Documents/Hadoop æ–‡ä»¶å¤¹é‡Œ 
```
cd Hadoop
tar -zxvf hadoop-2.8.5.tar.gz
```
ï¼ˆå®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬åœ¨ç»ˆç«¯é‡Œè§£å‹çš„2333ï¼‰

##### æ·»åŠ Hadoopç¯å¢ƒå˜é‡

åœ¨~/.bash_profileä¸­æ·»åŠ 
```
# Setting path for Hadoop
HADOOP_HOME="/Users/xusy/Documents/Hadoop/hadoop-2.8.5"
export HADOOP_HOME
export PATH=$PATH:HADOOP_HOME/sbin:$HADOOP_HOME/bin

export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native/
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native:$HADOOP_COMMON_LIB_NATIVE_DIR"
```
å…·ä½“è·¯å¾„æ ¹æ®hadoopçš„å®‰è£…ç›®å½•å†³å®š

ä¸‹åŠéƒ¨åˆ†çš„é…ç½®å¯ä»¥åœ¨ä¸Šé¢æåˆ°çš„ä¸€äº›

æ¥ä¸‹æ¥å¯ä»¥è¿›å…¥åˆ°æˆ‘ä»¬çš„Hadoopç›®å½•é‡Œ:

/hadoop-2.8.5/etc/hadoop/

ç„¶åä¿®æ”¹core-site.xml, mapred-site.xml(è¿™é‡Œæ˜¯mapred-site.xml.templateä¿®æ”¹æˆ.xml)

###### hadoop-env.sh

è¿™ä¸ªé…ç½®æ–‡ä»¶ç½‘ä¸Šæ‰¾åˆ°çš„å¤§éƒ¨åˆ†æ•™ç¨‹éƒ½è¦ä¿®æ”¹..ä½†æ˜¯..æˆ‘çœ‹å®Œæˆ‘ä¸‹è½½å®Œä¹‹åæ‰“å¼€çš„é»˜è®¤é…ç½®æ„Ÿè§‰ä¸ç”¨æ”¹..äºæ˜¯æ²¡æ”¹..

---æ›´æ–°---

åœ¨è¿™ä¸ªé…ç½®æ–‡ä»¶ä¸­åˆ æ‰äº†ä¸€äº›exportå‰çš„æ³¨é‡Š, å…³äºJAVA_HOME, JSVC_HOME, HADOOP_HOME, HADOOP_HEAPSIZE=1000(æˆ–è€…2000), HADOOP_OPTSä¸€äº›çš„æ³¨é‡Šéƒ½è¢«å»æ‰äº†ï¼Œæ— éœ€æ·»åŠ å•¥åˆ«çš„ä¸œè¥¿


---å†æ¥æ›´æ–°---

åœ¨åˆåˆåˆåˆå¯åŠ¨çš„æ—¶å€™å‘ç°è·‘ä»£ç çš„æ—¶å€™ä¼šæœ‰äº›é—®é¢˜..æŠ¥é”™ä¿¡æ¯æ˜¾ç¤ºçš„æ˜¯Javahomeçš„é—®é¢˜..ä»¥åŠHadoophomeçš„é—®é¢˜..å› æ­¤è¿˜æ˜¯å¯¹hadoop-env.shæ–‡ä»¶ä½œäº†ä¿®æ”¹ï¼Œå…·ä½“æ·»åŠ äº†javahome:
```
export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home"
export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"
export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS"

export HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_SECONDARYNAMENODE_OPTS"

export HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"
export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"
```
å…·ä½“çš„é…ç½®æ–‡ä»¶æ”¾åˆ°äº†æˆ‘çš„ GitHub -> HadoopClassNoteé‡Œï½


**åŒæ ·çš„ï¼š**åœ¨hadoop-env.sh, mapred-env.sh, yarn-env.shè¿™ä¸‰ä¸ªæ–‡ä»¶é‡Œéƒ½è¦å¯¹JAVA_HOMEè¿›è¡Œæ·»åŠ ä¿®æ”¹

###### core-site.xml

```
<configuration>
	<property>
		<name>fs.default.name</name>
		<value>hdfs://localhost:9000</value>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/Users/xusy/Documents/Hadoop</value>  ğŸ‘ˆğŸ¿æ˜¯è‡ªå®šä¹‰çš„æ”¾hdfsæ–‡ä»¶çš„ç›®å½•è¿™é‡Œæˆ‘å°±ç›´æ¥æ”¾åœ¨äº†æˆ‘çš„Hadoopç›®å½•é‡Œ
	</property>
</configuration>
```

(åæ¥ç”±äºnamenodeçš„ç›¸å…³ä¿¡æ¯å­˜åœ¨äº†ç³»ç»Ÿçš„tmpæ–‡ä»¶å¤¹é‡Œï¼Œå¯¼è‡´æ¯æ¬¡ç³»ç»Ÿé‡å¯çš„æ—¶å€™éƒ½ä¼šå‡ºç°é…ç½®ä¸èƒ½æˆåŠŸå¯åŠ¨ï¼Œæˆ‘ä»¬æ¯æ¬¡éƒ½è¦æ ¼å¼åŒ–namenodeï¼Œè¿™æ ·å°±éå¸¸ä¸okï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹è¿™ä¸ªæ–‡ä»¶ç¨å¾®ä¿®æ”¹äº†ä¸€ä¸‹)

```
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/Users/xusy/hadoop_tmp</value> 
	</property>
```

###### mapred-site.xml

è¿™ä¸ªæ–‡ä»¶å®é™…ä¸Šæˆ‘ä¸‹è½½å®Œçš„åç¼€æ˜¯.xml.template(è¿˜æ˜¯å•¥ç©æ„åæ­£æ˜¯åé¢æœ‰ä¸ªåç¼€ï¼Œè¢«æˆ‘ç›´æ¥ä¿®æ”¹æˆäº†.xml)
```
<configuration>
  <property>
    <name>mapred.job.tracker</name>
    <value>localhost:9010</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

###### hdfs-site.xml

```
<configuration>
	<!--ä¼ªåˆ†å¸ƒå¼-->
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
</configuration>
```
è¿™é‡Œçš„å˜é‡dfs.replicationæŒ‡å®šäº†æ¯ä¸ªHDFSæ•°æ®åº“çš„å¤åˆ¶æ¬¡æ•°ï¼Œé€šå¸¸ä¸º3ï¼Œè€Œæˆ‘ä»¬è¦åœ¨æœ¬æœºå»ºç«‹ä¸€ä¸ªä¼ªåˆ†å¸ƒå¼çš„DataNodeæ‰€ä»¥è¿™ä¸ªå€¼æ”¹æˆäº†1

ä¸ºäº†ä¿å­˜hdfsçš„å…ƒæ•°æ®å’Œdataç›¸å…³æ–‡ä»¶ï¼Œè¿™é‡Œåæ¥æ·»åŠ äº†propertyï¼š
```
<configuration>
	<!--ä¼ªåˆ†å¸ƒå¼-->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/Users/xusy/Documents/Hadoop/dfs/name</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/Users/xusy/Documents/Hadoop/dfs/data</value>
  </property>
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
  <property>
    <name>dfs.permissions</name>
    <value>false</value>
  </property>
</configuration>

```
###### yarn-site.xml

```
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>

<!-- Site specific YARN configuration properties -->

<!-- é›†ç¾¤é…ç½®-->
  <!--      <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>master</value>
      </property> -->

</configuration>
```
åŒæ ·çš„ç¨å¾®åšäº†ä¿®æ”¹
```
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>localhost:8031</value>
  </property>
    <property>
    <name>yarn.resourcemanager.address</name>
    <value>localhost:8032</value>
  </property>
    <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>localhost:8033</value>
  </property>
    <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>localhost:8034</value>
  </property>
    <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>localhost:8088</value>
  </property>
    <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
    <property>
    <name>yarn.log.server.url</name>
    <value>http://localhost:19888/jobhistory/logs/</value>
  </property>
<!-- Site specific YARN configuration properties -->

<!-- é›†ç¾¤é…ç½®-->
  <!--      <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>master</value>
      </property> -->
</configuration>
```

###### log4j.properties

åœ¨å…·ä½“è·‘ä»£ç çš„æ—¶å€™ä¼šæœ‰äº›WARNING(ä½†å®é™…ä¸Šä½ çš„ä»£ç å¹¶æ²¡æœ‰ä»€ä¹ˆé—®é¢˜..)å› æ­¤æˆ‘ä»¬è¦åœ¨log4j.propertiesæ–‡ä»¶åè¿½åŠ ä¸€è¡Œå†…å®¹ï¼š
```
log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR
```

##### å¯åŠ¨Hadoop

> æ¯æ¬¡æ“ä½œçš„æ—¶å€™éƒ½è¦è¿›å…¥è¿™ä¸ªHadoopæ–‡ä»¶å¤¹å“¦ï¼ˆå½“ç„¶æˆ‘è§‰å¾—å¦‚æœæŠŠè¿™ä¸ªæ·»åŠ åˆ°ç¯å¢ƒå˜é‡é‡Œä¼šä¸ä¼šå¥½ç‚¹..æˆ‘ä¹Ÿä¸çŸ¥é“æˆ‘çè¯´çš„

ç»ˆç«¯è¿›å…¥åˆ°Hadoopçš„æ–‡ä»¶å¤¹ä¸‹
æˆ‘è¿™é‡Œçš„æ–‡ä»¶å¤¹å°±æ˜¯
```
/Users/xusy/Documents/Hadoop/hadoop-2.8.5
```
æ‰§è¡Œ
```
./bin/hdfs namenode -format
```
æ ¼å¼åŒ–æ–‡ä»¶ç³»ç»Ÿï¼ˆå¯¹namenodeè¿›è¡Œåˆå§‹åŒ–)ï¼ˆå¥½åƒæ˜¯åªè¦åˆå§‹åŒ–ä¸€æ¬¡å°±å¥½äº†å°±æ˜¯æœ€å¼€å§‹å»ºç³»ç»Ÿçš„æ—¶å€™..ä¹‹åå¦‚æœæ¯æ¬¡å¯åŠ¨ä½ éƒ½åˆå§‹åŒ–..é‚£ä¹ˆæ˜¯ä¼šæœ‰é—®é¢˜çš„ï¼ï¼‰

---
æ›´æ–°

---

åœ¨å¯åŠ¨Hadoopï¼Œjpsä¹‹åå¯èƒ½ä¼šå‡ºç°ä½ çš„namenodeæ²¡èµ·æ¥çš„è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä¸ªæ—¶å€™å°±å¾—æ ¼å¼åŒ–ä¸€ä¸‹namenodeï¼Œå…·ä½“çš„è¯ğŸ‘‡ğŸ¿

è¿™é‡Œçš„namenode formatçš„é—®é¢˜ï¼šç”±äºnamenodeçš„ä¿¡æ¯æ˜¯å­˜åœ¨äº†ç³»ç»Ÿçš„tmpæ–‡ä»¶å¤¹ä¸‹çš„ï¼Œå¦‚æœä½ åˆ°è¿™é‡Œçœ‹çš„è¯æ˜¯èƒ½çœ‹è§è¿™äº›çš„ï¼š

![tmp](/img/tmp.png)

æ¯æ¬¡å¯åŠ¨çš„è¯tmpæ˜¯ä¼šæ¸…ç©ºçš„ï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“å’‹å›äº‹åæ­£ï¼Œè™½ç„¶æˆ‘åœ¨core-site.xmlæ–‡ä»¶é‡Œæ˜æ˜å®šä¹‰çš„æ˜¯tmpå­˜åœ¨äº†Hadoopæ–‡ä»¶å¤¹ä¸‹...ä½†è¿˜æ˜¯æœ‰è¿™ä¸ªé—®é¢˜..æ‰€ä»¥å°±é‡æ–°åœ¨æˆ‘çš„xusyç”¨æˆ·ä¸‹é¢æ–°å»ºäº†ä¸€ä¸ªhadoop_tmpæ–‡ä»¶å¤¹ï¼ŒæŠŠä¸Šé¢core-site.xmlé‡Œå­˜tempçš„é‚£ä¸ªæ–‡ä»¶å¤¹è·¯å¾„æ”¹æˆäº†
```
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/Users/xusy/hadoop_tmp</value> 
```
ç„¶åé‡æ–°formatå°±å¯ä»¥äº†..ä¸çŸ¥é“å†é‡æ–°å¯åŠ¨æˆ‘çš„ç”µè„‘çš„æ—¶å€™è¿˜ä¼šä¸ä¼šæœ‰è¿™ä¸ªé—®é¢˜..å¦‚æœæœ‰é‚£å°±å†æ›´æ–°ä¸€ä¸‹..		


æ¥ä¸‹æ¥å¯åŠ¨namenode & datanode ï¼ˆæ„Ÿè§‰å°±æ˜¯å¯åŠ¨dfsæ–‡ä»¶ç³»ç»Ÿ)
```
./sbin/start-dfs.sh
```
ä¸­é—´ä¼šæœ‰ä¸€ä¸ªè¯¢é—®yes/noçš„æˆ‘ä»¬è¾“å…¥yeså°±å¥½äº†..
å¯åŠ¨yarn
```
./sbin/start-yarn.sh
```
å¯åŠ¨æ—¥å¿—ç®¡ç†logçš„histroyserver 
```
./mr-jobhistory-daemon.sh start historyserver
```
ğŸ‘†ğŸ¿è¾“å…¥äº†è¿™ä¸ªå‘½ä»¤å°±å¯ä»¥åœ¨jpsé‡Œçœ‹è§JobHistoryServeräº†

å½“ç„¶ä»¥ä¸Šçš„å‘½ä»¤éƒ½æ˜¯åœ¨hadoop-2.8.5ä¸‹é¢è¿è¡Œçš„

æƒ³è¦å…³é—­çš„è¯..
```
./sbin/stop-all.sh
# stop-dfs.sh stop-yarn.sh
```

æŸ¥çœ‹å½“å‰çš„hadoopè¿è¡Œæƒ…å†µ:
```
xushiyaodeMacBook-Pro:sbin xusy$ jps
39696 SecondaryNameNode
39809 ResourceManager
49810 JobHistoryServer
39891 NodeManager
39507 NameNode
69306 
39595 DataNode
73471 Jps
```
æµ‹è¯•ä¸€ä¸‹æˆ‘ä»¬èƒ½ä¸èƒ½è¿›å…¥åˆ°overviewç•Œé¢å‘¢ï¼

NameNode - http://localhost:50070

ps:è¿™é‡Œæœ‰ä¸€ä¸ªHadoop2å’ŒHadoop3å¯¹åº”ç«¯å£ä¿®æ”¹çš„è¡¨åœ¨ä¸‹é¢ï¼š

NameNodeç«¯å£

| Hadoop2 | Hadoop3 |
| ------: | ------: |
|   50470 |    9871 |
|   50070 |    9870 |
|    8020 |    9820 |

Secondary NNç«¯å£

| Hadoop2 | Hadoop3 |
| ------: | ------: |
|   50091 |    9869 |
|   50090 |    9868 |

DataNodeç«¯å£

| Hadoop2 | Hadoop3 |
| ------: | ------: |
|   50020 |    9867 |
|   50010 |    9866 |
|   50475 |    9865 |
|   50075 |    9864 |

##### ç»§ç»­å¯åŠ¨ï¼ï¼ï¼

ç”±äºæˆ‘ä»¬åˆšåˆšåˆ°é…ç½®..è¿™é‡Œçš„namenode1å¯¹åº”çš„å°±æ˜¯æˆ‘ä»¬æœ¬æœºlocalhostå•¦ï½(æ‰€ä»¥ä¸‹é¢çš„webæŸ¥çœ‹æ­£å¸¸è¾“å…¥çš„URLåº”è¯¥æ˜¯namenode1+ç«¯å£çš„)

overviewæŸ¥çœ‹ï¼

æŸ¥çœ‹HDFSï¼š

http://localhost:50070

æŸ¥çœ‹YARNï¼š

http://localhost:8088

æŸ¥çœ‹MRå¯åŠ¨JobHistory Server(è¿™é‡Œæš‚æ—¶å‡ºäº†é—®é¢˜..è®©æˆ‘ç ”ç©¶ä¸€ä¸‹..)

http://localhost:19888

### 2. Hive

ä¸€ã€å®‰è£… **MySQL**

1. ä¸Šä¼ MySQLåœ¨çº¿å®‰è£…æºçš„é…ç½®æ–‡ä»¶

ç”¨WinSCPï¼ˆrootè´¦å·è¿æ¥ï¼‰CentOSæœåŠ¡å™¨

å°†mysql-community.repo æ–‡ä»¶ä¸Šä¼ åˆ° /etc/yum.repos.d/ ç›®å½•

å°†RPM-GPG-KEY-mysql æ–‡ä»¶ä¸Šä¼ åˆ° /etc/pki/rpm-gpg/ ç›®å½•

 

2. æ›´æ–°yumæºå¹¶å®‰è£…mysql serverï¼ˆé»˜è®¤åŒæ—¶ä¼šå®‰è£…mysql clientï¼‰

> yum repolist

> yum install mysql-server

 

3. æŸ¥çœ‹MySQLå„ç»„ä»¶æ˜¯å¦æˆåŠŸå®‰è£…

> rpm -qa | grep mysql

![img](http://ww1.sinaimg.cn/large/006tNc79ly1g4lo4au6h4j308j01ngm8.jpg) 

 

 

äºŒã€é…ç½®**MySQL**

1. å¯åŠ¨MySQL Serverå¹¶æŸ¥çœ‹å…¶çŠ¶æ€

> systemctl start mysqld

> systemctl status mysqld

![img](http://ww4.sinaimg.cn/large/006tNc79ly1g4lo815xcsj30dz028gmn.jpg)ã€

2. æŸ¥çœ‹MySQLç‰ˆæœ¬

> mysql -V

![img](http://ww2.sinaimg.cn/large/006tNc79ly1g4lo83tyhbj30dz00mweq.jpg) 

 

3. è¿æ¥MySQLï¼Œé»˜è®¤rootå¯†ç ä¸ºç©º

> mysql -u root   (è¿™ä¸ªå‘½ä»¤ä¸å¥½ç”¨ï¼Œç”¨ mysql -u root -p )

> mysql> s

è¿™é‡Œå¦‚æœä½¿ç”¨ > myswl -u root ä¼šæŠ¥ä»¥ä¸‹é”™è¯¯

> ERROR 1044 (42000): Access denied for user ''@'localhost' to database 'mysql' 

4. æŸ¥çœ‹æ•°æ®åº“

> mysql> show databases; ï¼ˆæ³¨æ„ï¼šå¿…é¡»ä»¥åˆ†å·ç»“å°¾ï¼Œå¦åˆ™ä¼šå‡ºç°ç»­è¡Œè¾“å…¥ç¬¦â€œ>â€ï¼‰

 

5. åˆ›å»ºhiveå…ƒæ•°æ®æ•°æ®åº“ï¼ˆmetastoreï¼‰

> mysql> create database hive; 

![img](http://ww4.sinaimg.cn/large/006tNc79ly1g4lo9qcxrpj308o04z0tu.jpg) 

 

6. åˆ›å»ºç”¨æˆ·hiveï¼Œå¯†ç æ˜¯123456

> mysql> CREATE USER 'hive'@'%' IDENTIFIED BY '123456';

æ³¨æ„ï¼šåˆ é™¤ç”¨æˆ·æ˜¯DROP USERå‘½ä»¤ 

 

7. æˆæƒç”¨æˆ·hadoopæ‹¥æœ‰æ•°æ®åº“hiveçš„æ‰€æœ‰æƒé™

mysql> GRANT ALL PRIVILEGES ON hive.* TO 'hive'@'%' WITH GRANT OPTION;

 

8. æŸ¥çœ‹æ–°å»ºçš„MySQLç”¨æˆ·ï¼ˆæ•°æ®åº“åï¼šmysqlï¼Œè¡¨åï¼šuserï¼‰

> mysql> select host,user,password from mysql.user;

![img](http://ww1.sinaimg.cn/large/006tNc79ly1g4lo9tbz18j30dz04imyu.jpg) 

 

9. åˆ é™¤ç©ºç”¨æˆ·è®°å½•ï¼Œå¦‚æœæ²¡åšè¿™ä¸€æ­¥ï¼Œæ–°å»ºçš„hiveç”¨æˆ·å°†æ— æ³•ç™»å½•ï¼Œåç»­æ— æ³•å¯åŠ¨hiveå®¢æˆ·ç«¯

> mysql> delete from mysql.user where user='';

 

10. åˆ·æ–°ç³»ç»Ÿæˆæƒè¡¨ï¼ˆä¸ç”¨é‡å¯mysqlæœåŠ¡ï¼‰

> mysql> flush privileges; 

 

11. æµ‹è¯•hiveç”¨æˆ·ç™»å½•

> mysql -u hive -p

> Enter passwordï¼š123456



**ä¸‰ã€å®‰è£…å’Œé…ç½®hive**

1. ä¸‹è½½hive

> Wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz

2. è§£å‹hive-1.1.0-cdh5.12.1.tar.gzåˆ°/home/hadoop

> $ tar zxvf apache-hive-2.3.5-bin.tar.gz

 

3. åœ¨.bash_profileæ–‡ä»¶ä¸­æ·»åŠ hiveç¯å¢ƒå˜é‡

> export HIVE_HOME=/home/hadoop/hive-1.1.0-cdh5.12.1

> export PATH=$HIVE_HOME/bin:$PATH

4. ä½¿ä¸Šè¿°è®¾ç½®ç”Ÿæ•ˆ

   > $ source .bash_profile

 

5. ç¼–è¾‘$HIVE_HOME/conf/hive-env.shæ–‡ä»¶ï¼Œåœ¨æœ«å°¾æ·»åŠ HADOOP_HOMEå˜é‡

> cd $HIVE_HOME/conf

> cp hive-env.sh.template hive-env.sh	ï¼ˆé»˜è®¤ä¸å­˜åœ¨ï¼Œå¯ä»æ¨¡æ¿æ–‡ä»¶å¤åˆ¶ï¼‰

> vi hive-env.sh

> HADOOP_HOME=/root/Hadoop/hadoop-2.8.5

 

6. æ–°å»º$HIVE_HOME/conf/hive-site.xmlæ–‡ä»¶

```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://localhost:3306/hive</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>hive</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>123456</value>
        </property>

		<property>
				<name>hive.metastore.warehouse.dir</name>
				<value>/hive/warehouse</value>
		</property>
		<property>
				<name>hive.exec.scratchdir</name>
				<value>/hive/tmp </value>
		</property>
        <property>
                <name>hive.metastore.schema.verification</name>
                <value>false</value>
        </property>
</configuration>
```



 

7. åœ¨HDFSä¸Šåˆ›å»ºæ•°æ®ä»“åº“ç›®å½•ï¼ˆç”¨äºå­˜æ”¾hiveæ•°æ®æ–‡ä»¶ï¼‰å’Œä¸´æ—¶ç›®å½•

> hdfs dfs -mkdir -p /hive/warehouse /hive/tmp

 

8. ä¸‹è½½mysqlè¿æ¥é©±åŠ¨ï¼Œä¸‹è½½åœ°å€ï¼šhttps://dev.mysql.com/downloads/connector/j/

![img](file:////var/folders/nm/nfxnvn057nq5rsjjdhz11rsw0000gn/T/com.kingsoft.wpsoffice.mac/wps-bellick/ksohtml/wpsJYMEDH.png) 

	ä¸‹è½½æ–‡ä»¶(.tar.gz)è§£å‹åï¼Œå°†å…¶ä¸­çš„mysql-connector-java-8.0.13.jaræ–‡ä»¶ä¸Šä¼ åˆ° $HIVE_HOME/libç›®å½•ä¸‹

 


9. å¯åŠ¨hive

> hive

10. æŸ¥çœ‹hiveæ•°æ®åº“ ï¼ˆæ³¨æ„ï¼šå‘½ä»¤ä»¥åˆ†å·ç»“å°¾ï¼‰

> hive> show databases;

![img](file:////var/folders/nm/nfxnvn057nq5rsjjdhz11rsw0000gn/T/com.kingsoft.wpsoffice.mac/wps-bellick/ksohtml/wpsY2lKug.jpg) 

defaultæ˜¯é»˜è®¤æ•°æ®åº“

11. é€€å‡ºhive

> hive> quit;

##### Hive> Show databases; æŠ¥é”™

> hive> show databases;
> FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If needed, don't forget to include the option to auto-create the underlying database in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql))



åœ¨HIVE_HOME/conf/hive-site.xml ä¸­æ·»åŠ å¦‚ä¸‹é…ç½®

```
<property>
<name>datanucleus.schema.autoCreateAll</name>
<value>true</value>
</property>
```

### 3. Impala 

1. å…ˆå»http://archive.cloudera.com/beta/impala-kudu/redhat/7/x86_64/impala-kudu/0/RPMS/x86_64/ä¸‹è½½æ‰€éœ€çš„åŒ…
2. ä¾æ¬¡å®‰è£…è¿™äº›åŒ…

```shell
rpm -ivh bigtop-utils-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
```

3. impala é…ç½®

   3.1 æ·»åŠ hadoopå®‰è£…ç›®å½•ä¸‹çš„core-site.xml,hdfs.xml å’Œ hiveçš„hive-site.xml åˆ°/etc/impala/conf 

   3.2 ä¿®æ”¹æ–‡ä»¶ /etc/default/bigtop-utils ï¼Œæ–°å¢java_homeè·¯å¾„ï¼›

   3.3 ä¿®æ”¹æ–‡ä»¶ /etc/default/impalaï¼Œåªéœ€ä¿®æ”¹å‰ä¸¤è¡Œï¼Œæ”¹ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€æˆ–è€…hostname, è‹¥/etc/hostsæ–‡ä»¶é…ç½®äº† 127.0.0.1 localhost ï¼Œä¹Ÿå¯ä¸åšä¿®æ”¹

   3.4 ä¿®æ”¹core-site.xmlï¼Œæ–°å¢ä»¥ä¸‹å‡ é¡¹:

   ```
   <property>
           <name>dfs.client.read.shortcircuit</name>
           <value>true</value>
   </property>
   <property>
           <name>dfs.client.read.shortcircuit.skip.checksum</name>
           <value>false</value>
   </property>
   <property>
           <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
           <value>true</value>
   </property>
   ```

   3.5 ä¿®æ”¹hdfs-site.xmlï¼Œæ–°å¢ä»¥ä¸‹å‡ é¡¹:

   ```xml
   <property>
           <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
           <value>true</value>
   </property>
   <property>
           <name>dfs.block.local-path-access.user</name>
           <value>impala</value>
   </property>
   <property>
           <name>dfs.client.file-block-storage-locations.timeout.millis</name>
           <value>60000</value>
   </property>
   ```

   3.6 æƒé™é…ç½®

   > 1. sermod -G hdfs,hadoop impala
   > 2. groups impala

   3.7 åˆ›å»ºimpalaåœ¨hdfsç›®å½•ï¼Œèµ‹äºˆæƒé™(å•èŠ‚ç‚¹å³å¯)ï¼š

   > 1.  hdfs dfs -mkdir /user/impala
   > 2.  hadoop fs -chown impala /user/impala*

4. å¯åŠ¨impala ä¹‹å‰ï¼Œå…ˆå¯åŠ¨hadoop ,hiveserver2çš„æœåŠ¡(è‹¥é…ç½®äº†ï¼Œå¦åˆ™å¯åŠ¨hiveserveræœåŠ¡)

5. å¯åŠ¨impalaæœåŠ¡,  ä¸»æœºèŠ‚ç‚¹å³å¯ï¼Œä»æœºå¯ä»¥ä¸å¯åŠ¨impala-serveræœåŠ¡,æ‰€ç¤ºçš„ipä¸ºåˆšæ‰é…ç½®æ–‡ä»¶æ‰€é…çš„ipæˆ–è€…ä¸ºipå¯¹åº”çš„ hostnameï¼Œæœªä¿®æ”¹åˆ™ä¸º127.0.0.1ï¼š

   ```shell
   [root@master run]# service impala-state-store restart --kudu_master_hosts=192.168.174.132:7051
   Stopped Impala State Store Server:                         [  ç¡®å®š  ]
   Started Impala State Store Server (statestored):           [  ç¡®å®š  ]
   [root@master run]# service impala-catalog restart --kudu_master_hosts=192.168.174.132:7051
   Stopped Impala Catalog Server:                             [  ç¡®å®š  ]
   Started Impala Catalog Server (catalogd) :                 [  ç¡®å®š  ]
   [root@master run]# service impala-server restart --kudu_master_hosts=192.168.174.132:7051
   Stopped Impala Server:                                     [  ç¡®å®š  ]
   Started Impala Server (impalad):                           [  ç¡®å®š  ]
   [root@master run]# 
   ```

6. å¯åŠ¨ impala-shell

> åŸºäº https://blog.csdn.net/qq_41792743/article/details/87979146

## éœ€æ±‚åˆ†æ

1. äº‹ä»¶åˆ†æ

   * ç”¨æˆ·åœ¨äº§å“ä¸Šçš„è¡Œä¸ºæˆ‘ä»¬å®šä¹‰ä¸ºäº‹ä»¶ï¼Œå®ƒæ˜¯ç”¨æˆ·è¡Œä¸ºçš„ä¸€ä¸ªä¸“ä¸šæè¿°ï¼Œç”¨æˆ·åœ¨äº§å“ä¸Šçš„æ‰€æœ‰è·å¾—çš„ç¨‹åºåé¦ˆéƒ½å¯ä»¥æŠ½è±¡ä¸ºäº‹ä»¶è¿›è¡Œé‡‡é›†ã€‚äº‹ä»¶å¯ä»¥é€šè¿‡åŸ‹ç‚¹ã€é€šè¿‡å¯è§†åŒ–åœˆé€‰ç”Ÿæ•ˆï¼Œæ­¤æ–‡æ¡£ä»¥åŸ‹ç‚¹é‡‡é›†ä¸ºä¸»ã€‚å½“ç„¶ï¼Œä½ å¯ä»¥è‡ªå®šä¹‰äº‹ä»¶çš„åç§°ã€å±æ€§çš„åç§°ä»¥åŠä¸ªæ•°
   * åˆ†æå•ä¸ªäº‹ä»¶éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿ã€‚
   * æ ¹æ®äº‹ä»¶çš„æŸä¸ªæŒ‡æ ‡è§‚å¯Ÿå˜åŒ–è¶‹åŠ¿
   * æ ¹æ®ç”¨æˆ·å±æ€§æˆ–äº‹ä»¶å±æ€§è¿›è¡Œ**åˆ†ç»„å¯¹æ¯”**ï¼›

2. æ¼æ–—åˆ†æ

   * æ¼æ–—æ¨¡å‹ä¸»è¦ç”¨äºåˆ†æä¸€ä¸ªå¤šæ­¥éª¤è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„è½¬åŒ–ä¸æµå¤±æƒ…å†µã€‚

   * é€‰æ‹©éœ€è¦åˆ†æçš„æ—¥æœŸ

     ç”¨æˆ·å¯ä»¥é€‰æ‹©éœ€è¦åˆ†æçš„èµ·å§‹æ—¶é—´

   * ç‚¹å‡»åˆ›å»ºæ¼æ–—

     ç”¨æˆ·å¯ä»¥è‡ªå·±é€‰æ‹©åˆ›å»ºè‹¥å¹²æ¼æ–—è¿‡ç¨‹ã€‚

   * æ¼æ–—å›¾å±•ç¤º

     ç”¨æˆ·é€‰æ‹©æ—¶é—´å’Œæ¼æ–—åç‚¹å‡»æäº¤ï¼Œç³»ç»Ÿä¼šä¸ºç”¨æˆ·ç”»å‡ºæ¼æ–—å›¾ï¼Œå›¾ä¸­æ ‡è®°å‡ºæ¯ä¸ªè¿‡ç¨‹çš„ç”¨æˆ·æ•°ï¼Œç›¸é‚»æ¼æ–—çš„é¢ç§¯å¯¹æ¯”å³æ˜¯è¯¥è¿‡ç¨‹çš„è½¬åŒ–ç‡ã€‚

3. ç•™å­˜åˆ†æ

   * ç”¨æˆ·é€‰æ‹©åˆ†æçš„æ—¶é—´æ®µ

     ç”¨æˆ·å¯ä»¥è‡ªä¸»é€‰æ‹©åˆ†æçš„èµ·æ­¢æ—¶é—´ï¼Œç²’åº¦ä¸ºæ—¥

   * ç”¨æˆ·é€‰æ‹©åˆå§‹è¡Œä¸º

     åˆå§‹è¡Œä¸ºé€‰æ‹©ç”¨æˆ·åªè§¦å‘ä¸€æ¬¡çš„äº‹ä»¶ï¼Œæ¯”å¦‚â€œæ³¨å†Œâ€ã€â€œä¸Šä¼ å¤´åƒâ€ã€â€œæ¿€æ´»è®¾å¤‡â€ç­‰ã€‚

   * ç”¨æˆ·é€‰æ‹©åç»­è¡Œä¸º

     åç»­è¡Œä¸ºé€‰æ‹©ä½ æœŸæœ›ç”¨æˆ·é‡å¤è§¦å‘çš„äº‹ä»¶ï¼Œæ¯”å¦‚â€œé˜…è¯»æ–‡ç« â€ã€â€œå‘å¸–â€ã€â€œè´­ä¹°â€ç­‰ã€‚è¿™ç§ç•™å­˜ç”¨äºå¯¹æ¯”åˆ†æä¸åŒé˜¶æ®µå¼€å§‹ä½¿ç”¨äº§å“çš„æ–°ç”¨æˆ·çš„å‚ä¸æƒ…å†µï¼Œä»è€Œè¯„ä¼°äº§å“è¿­ä»£æˆ–è¿è¥ç­–ç•¥è°ƒæ•´çš„å¾—å¤±ã€‚

4. åŠŸèƒ½å±•ç¤º

   * ç”¨æˆ·é€šè¿‡ç½‘é¡µè¡¨å•é€‰æ‹©åŠŸèƒ½éœ€æ±‚
   * åç«¯æ¥æ”¶ç½‘é¡µä¼ æ¥çš„æ•°æ®

5. éœ€æ±‚å½’çº¦

6. æ•°æ®å­—å…¸

## æ•°æ®å¯¼å…¥

å°†æ•°æ®æ–‡ä»¶æ‹·è´åˆ°HDFSä¸Šï¼Œç„¶åå»ºç«‹ä¸€å¼ impalaå¤–éƒ¨è¡¨ï¼Œå°†å¤–éƒ¨è¡¨çš„å­˜å‚¨ä½ç½®ï¼ŒæŒ‡å‘æ•°æ®æ–‡ä»¶

![IMG_996067991597-1](http://ww1.sinaimg.cn/large/006tNc79ly1g53ojhwe48j31kw0r74qp.jpg)

1. ç”¨scpå°†æ•°æ®æ–‡ä»¶ä¼ åˆ°æœåŠ¡å™¨

2. åœ¨HDFSä¸Šå»ºç«‹å­˜å‚¨æ•°æ®çš„ç›®å½•

   > su hdfs
   >
   > hdfs dfs -mkdir -p  /user/impala/data /user/impala/data/event_export /user/impala/data/user_export

3. ä¿®æ”¹HDFSç›®å½•æƒé™ï¼ˆå¦‚æœéœ€è¦ï¼‰

   > hdfs dfs -chmod 777 /user/impala/data/event_export 

4. å°†æ•°æ®æ–‡ä»¶ä¼ åˆ°HDFSæŒ‡å®šç›®å½•ä¸Š

   > hdfs dfs -put /home/work/event_export/xxxxxx.xxx  /user/impala/data/event_export 

   >  hdfs dfs -put /home/work/user_export/xxxxxx.xxx  /user/impala/data/user_export 

5. åœ¨impala-shellä¸­å»ºç«‹å¤–éƒ¨è¡¨ï¼Œå¹¶æŒ‡å‘æ•°æ®æ–‡ä»¶

   > Impala-shell > CREATE TABLE rawdata.event_export (
   >   event_id INT,
   >   month_id INT,
   >   week_id INT,
   >   user_id BIGINT,
   >   distinct_id STRING,
   >   time TIMESTAMP,
   >   day INT,
   >   event_bucket INT,
   >   _offset BIGINT,
   >   p__app_version STRING,
   >   ...
   > )
   > STORED AS TEXTFILE
   > LOCATION '/user/impala/data/event_export '

## æ€»ä½“è®¾è®¡

#### 1. äº‹ä»¶åˆ†æ

1. ç”¨æˆ·é€‰æ‹©æ—¶é—´æ®µ 

2. ç”¨æˆ·é€‰æ‹©äº‹ä»¶ï¼ˆè¡Œä¸ºï¼‰-> äº‹ä»¶ä¸‹æ‹‰æ¡†

3. ç”¨æˆ·é€‰æ‹©äº‹ä»¶çš„å±•ç¤ºæŒ‡æ ‡ -> æŒ‡æ ‡ä¸‹æ‹‰æ¡†ï¼ˆ5ä¸ªæŒ‡æ ‡maxï¼‰-> æŒ‡æ ‡é€šè¿‡å­—å…¸æ˜ å°„åˆ°sql

4. ç”¨æˆ·é€‰æ‹©æŒ‰æŸç§æŒ‡æ ‡åˆ†ç»„

   3.1 å±•ç¤ºæŒ‡æ ‡ï¼šæ€»æ¬¡æ•°ã€æ€»äººæ•°ã€å»é‡äººæ•°ã€äººå‡æ¬¡æ•°ã€å¹³å‡äº‹ä»¶æ—¶é•¿ã€
   4.1 åˆ†ç»„æŒ‡æ ‡ï¼šå¹¿å‘Šç³»åˆ—æ¥æº -> æ¥æºåˆ†æå¯å¸®åŠ©ç”¨æˆ·è¿›è¡Œå¹¿å‘ŠæŠ•æ”¾ã€æ˜¯å¦é¦–æ¬¡è®¿é—®

#### 2. æ¼æ–—åˆ†æ

æ¼æ–—æµç¨‹ï¼š

ğŸŒ°ï¼šç‚¹å‡»å¿˜è®°å¯†ç id=5 -> æ‰¾å›å¯†ç -è·å–éªŒè¯ç id=19 -> æ‰¾å›å¯†ç -é‡ç½®å¯†ç id=28 -> æäº¤æ–°å¯†ç id=1

1. ç”¨æˆ·é€‰æ‹©éœ€è¦æŸ¥è¯¢è¿‡æ»¤çš„å¹´ï¼Œæœˆ
2. ç”¨æˆ·æŒ‰é¡ºåºé€‰æ‹©éœ€è¦è¿‡æ»¤çš„æµç¨‹ï¼ˆ4æ­¥ï¼‰
3. è¿”å›æœ¬æœˆä¸­å¯¹åº”æµç¨‹çš„äººæ•°å’Œè½¬åŒ–æ¯”ä¾‹

#### 3. ç•™å­˜åˆ†æ

1. ç”¨æˆ·é€‰æ‹©æ—¶é—´æ®µ

2. ç”¨æˆ·åˆå§‹è¡Œä¸º

3. ç”¨æˆ·é€‰æ‹©åç»­è¡Œä¸º

4. å±•ç¤ºæ—¶é—´æ®µå†…7å¤©ç•™å­˜çš„ç»“æœåˆ†æï¼šæ€»äººæ•°ï¼Œ1å¤©ä¹‹å†…æ¯”ä¾‹ï¼Œç¬¬äºŒå¤©æ¯”ä¾‹...ç¬¬ä¸ƒå¤©æ¯”ä¾‹

   4.1è¿”å›çš„ç»“æ„æ˜¯ä¸€å¼ ä»from_timeåˆ°to_timeè¿™ä¹ˆå¤šè¡Œï¼Œæ¯è¡Œå…ƒç´ æ˜¯æ€»äººæ•°ï¼Œ1å¤©ï¼Œ2å¤©...ç¬¬ä¸ƒå¤©æ¯”ä¾‹ è¿™ä¹ˆå¤šåˆ—çš„è¡¨

## ç•Œé¢è®¾è®¡

æŒ‰ç…§ç¥ç­–çš„æ–‡æ¡£ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªé˜‰å‰²ç‰ˆçš„ç•Œé¢

* å¯¹äºäº‹ä»¶åˆ†æï¼Œæˆ‘ä»¬å…è®¸ç”¨æˆ·é€‰æ‹©
  * äº‹ä»¶çš„æ—¶é—´åŒºé—´
  * åˆ†ææŒ‡æ ‡
  * åˆ†ç»„å±•ç¤ºæ–¹å¼

![](http://ww1.sinaimg.cn/large/006tNc79ly1g50gaf712xj30vq0a0mye.jpg)

* å¯¹äºç•™å­˜åˆ†æï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©
  * äº‹ä»¶èµ·æ­¢æ—¥æœŸ
  * ç”¨æˆ·åˆå§‹è¡Œä¸º
  * ç”¨æˆ·åç»­è¡Œä¸º

![](http://ww1.sinaimg.cn/large/006tNc79ly1g50gaqm0v0j30vq08i75d.jpg)

* å¯¹äºæ¼æ–—åˆ†æï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©
  * å¹´ä»½
  * æœˆä»½
  * æ„æˆæ¼æ–—çš„è¡Œä¸º X 4

![](http://ww2.sinaimg.cn/large/006tNc79ly1g50g9yya1gj30vq0fxdgv.jpg)

## åŠŸèƒ½å®ç°

æˆ‘ä»¬åŸºäºimpyla åŒ…å®ç°ç”¨Pythonè¿æ¥impalaï¼Œåœ¨Pythonä¸­ç¼–è¾‘impala-SQLè¯­å¥ï¼Œé€šè¿‡è¿œç¨‹æäº¤æŸ¥è¯¢è¯·æ±‚æ¥ä½¿impalaåšå‡ºå“åº”ã€‚

#### 1. äº‹ä»¶åˆ†æ

#### 2. æ¼æ–—åˆ†æ

#### 3. ç•™å­˜åˆ†æ

1. ç”¨æˆ·åœ¨é¡µé¢é€‰æ‹©

   * æ—¶é—´æ®µ( yyyy-mm-dd,yyyy-mmâ€”dd) 

   * åˆå§‹äº‹ä»¶ ï¼š event-id 
   * åç»­äº‹ä»¶: event-id

2. æˆ‘ä»¬é¦–å…ˆè¦å°†å­—ç¬¦ä¸²çš„æ—¶é—´æ ¼å¼è½¬æ¢æˆUnixTimestamp

```python
from_time += " 00:00:00"
    to_time += " 00:00:00"
    from_time = time.strptime(from_time, "%Y-%m-%d %H:%M:%S")
    from_day = str(int(time.mktime(from_time) // 86400))
    to_time = time.strptime(to_time, "%Y-%m-%d %H:%M:%S")
    to_day = str(int(time.mktime(to_time) // 86400))
```

3. åœ¨è¡¨ä¸­æŸ¥è¯¢æ‰€æœ‰åœ¨è§„å®šæ—¶é—´æ®µå†…è¿›è¡Œè¿‡åˆå§‹äº‹ä»¶çš„ç”¨æˆ·ï¼Œå¹¶ä¸ºä»–ä»¬åˆ›å»ºä¸€ä¸ªä¸´æ—¶è¡¨user_init_event

```python
"with user_init_event " \
                    "as (select user_id, day as init_day " \
                    "from event_export_partition_parquet_g7 " \
                    "where event_id = "+ event_init +" and day >= "+from_day+" and day <= "+to_day+" ),"
```

4. å°†äº‹ä»¶è¡¨å’Œuser_init_eventè¡¨æŒ‰ç…§user_id  joinï¼Œå¹¶ç­›é€‰å‡ºå…¶ä¸­äº‹ä»¶ä¸ºåç»­äº‹ä»¶å¹¶ä¸”åç»­äº‹ä»¶å’Œåˆå§‹äº‹ä»¶çš„æ—¶é—´é—´éš”åœ¨0-7å¤©ï¼ŒæŠŠè¿™äº›ç”¨æˆ·çš„id,å‘ç”Ÿåˆå§‹äº‹ä»¶çš„æ—¶é—´ï¼Œæ—¶é—´é—´éš” å­˜åˆ°ä¸´æ—¶è¡¨ user_cohort ä¸­ã€‚

```python
"user_cohort as( " \
                    "select e.user_id,i.init_day,(e.day-i.init_day) as cohort_day " \
                    "from event_export_partition_parquet_g7 e LEFT JOIN user_init_event i on e.user_id = i.user_id " \
                    "where e.event_id = "+ event_remain+ " and (e.day-i.init_day)<7 and (e.day-i.init_day)>=0 " 
      							"group by user_id,cohort_day,i.init_day)" \
```

5. åœ¨user_cohortè¡¨ä¸­ ï¼ŒæŒ‰ç…§åˆå§‹äº‹ä»¶çš„æ—¶é—´ å’Œ ç•™å­˜æ—¶é—´åˆ†ç»„ å¹¶ä»¥åˆå§‹æ—¶é—´å’Œç•™å­˜æ—¶é—´æ’åºï¼Œè®¡ç®—æ¯ç»„ä¸­çš„äººæ•°ã€‚

```python
"select count(*),cohort_day,init_day from user_cohort group by init_day,cohort_day order by init_day,cohort_day"
```

6. æ€»çš„å‡½æ•°

```sql
def remain2(from_time,to_time,event_init,event_remain):
    from_time += " 00:00:00"
    to_time += " 00:00:00"
    from_time = time.strptime(from_time, "%Y-%m-%d %H:%M:%S")
    from_day = str(int(time.mktime(from_time) // 86400))
    to_time = time.strptime(to_time, "%Y-%m-%d %H:%M:%S")
    to_day = str(int(time.mktime(to_time) // 86400))

    cur.execute("use rawdata")
    create_string = "with user_init_event " \
                    "as (select user_id, day as init_day " \
                    "from event_export_partition_parquet_g7 " \
                    "where event_id = "+ event_init +" and day >= "+from_day+" and day <= "+to_day+" )," \
                    "user_cohort as( " \
                    "select e.user_id,i.init_day,(e.day-i.init_day) as cohort_day " \
                    "from event_export_partition_parquet_g7 e LEFT JOIN user_init_event i on e.user_id = i.user_id " \
                    "where e.event_id = "+ event_remain+ " and (e.day-i.init_day)<7 and (e.day-i.init_day)>=0 " \
                    "group by user_id,cohort_day,i.init_day)" \
                    "select count(*),cohort_day,init_day from user_cohort group by init_day,cohort_day order by init_day,cohort_day"

    start = datetime.datetime.now()
    cur.execute(create_string)
    res = cur.fetchall()
    end = datetime.datetime.now()

    print(res)
    print(end - start)
```

4. webåç«¯

   ä¸ºäº†æ–¹ä¾¿å±•ç¤ºï¼Œæˆ‘ä»¬é‡‡ç”¨webé¡µé¢çš„æ–¹å¼å‘ç”¨æˆ·æä¾›æœåŠ¡ã€‚ç”¨æˆ·å¯ä»¥åœ¨ç½‘é¡µä¸Šè¿›è¡Œè®¾ç½®ä»¥é€‰æ‹©è‡ªå·±éœ€è¦çš„æœåŠ¡å½¢å¼ã€‚

   å…·ä½“å®ç°æ–¹æ³•ä¸ºåŸºäºDjangoæ¨¡æ¿å¼•æ“çš„Pythonæ–¹æ³•ã€‚æˆ‘ä»¬ä¸ºç”¨æˆ·åˆ›å»ºfunnel,event,remainä¸‰ä¸ªé¡µé¢ã€‚åˆ†åˆ«å¯¹åº”æ¼æ–—åˆ†æï¼Œäº‹ä»¶åˆ†æï¼Œç•™å­˜åˆ†æã€‚ç”¨æˆ·åœ¨åœ°å€æ è¾“å…¥ç›¸åº”URLï¼Œç”¨æˆ·è¾“å…¥ä½œä¸ºPOSTæŠ¥æ–‡å†…å®¹ä¼ è‡³åç«¯ï¼Œåç«¯æ ¹æ®urlå°†è·¯ç”±åˆ†å‘åˆ°ç›¸åº”çš„å¤„ç†æ¨¡å—ã€‚å¤„ç†æ¨¡å—å¤„ç†ç”¨æˆ·POSTæŠ¥æ–‡ä¸­çš„å‚æ•°ä¿¡æ¯ã€‚å¹¶å°†è¿™äº›ä¿¡æ¯ä½œä¸ºå‚æ•°è°ƒç”¨ç›¸åº”çš„æŸ¥è¯¢æ–¹æ³•å‘é€åˆ°impalaæœåŠ¡å™¨ä»¥è·å¾—æ­£ç¡®çš„æŸ¥è¯¢ç»“æœã€‚

   ![å±å¹•å¿«ç…§ 2019-07-18 ä¸Šåˆ9.34.28](http://ww2.sinaimg.cn/large/006tNc79ly1g53qpm3l2gj30zs0kt4qp.jpg)

5. å¯è§†åŒ–ã€‚

   æ¼æ–—å›¾å’Œäº‹ä»¶åˆ†æçš„å›¾è¡¨é€šè¿‡pyechartsç»˜åˆ¶ã€‚è°ƒç”¨pyechartsåŒ…é‡Œçš„Lineç»˜åˆ¶æŠ˜çº¿å›¾ï¼ŒFunnelç»˜åˆ¶æ¼æ–—å›¾ï¼Œ

   ![funnel](http://ww2.sinaimg.cn/large/006tNc79ly1g53qsvhlpzj31980lgdhf.jpg)

   ![event](http://ww4.sinaimg.cn/large/006tNc79ly1g53qthixy0j31880mjdii.jpg)

## ä¼˜åŒ–æ–¹æ³•

1. å­˜å‚¨æ–¹å¼

   å°†TEXTæ•°æ®è½¬æ¢æˆParquetå­˜å‚¨

2. åˆ†åŒº

   å°†æ•°æ®æŒ‰ç…§ (day, event_bucket )åˆ†åŒº

> create table rawdata.parquet_partiton(
>
> xxx
>
>  ) 
>
> select ( xxxx ,day, event_bucket) from xxx
>
>  stored as parquet 
>
> partitoned by(day,event_bucket)

3. åœ¨SQLè¯­å¥ä¸­ï¼Œæ—¶é—´ç»´åº¦ä¸Šçš„ç­›é€‰æˆ‘ä»¬å°½é‡åœ¨ç”¨day æ¥ä½œä¸ºæŸ¥è¯¢æ¡ä»¶ï¼Œä»¥æé«˜æŸ¥è¯¢æ•ˆç‡ã€‚

## æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ

## æ­£ç¡®æ€§æµ‹è¯•

## æ€§èƒ½æµ‹è¯•
