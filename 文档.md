<br><br><br><br><br><br>

<h1  align=center>ã€Šä¸“ä¸šæ–¹å‘è¯¾ç¨‹è®¾è®¡ã€‹å¤§ä½œä¸š</h1>


<br><br><br><br><br>

<h3  align=center>é¢˜ç›®ï¼šå¤§è§„æ¨¡ç”¨æˆ·è¡Œä¸ºåˆ†æç³»ç»Ÿ</h3>


<br><br><br><br>



| å§“å  | å­¦å·  | ç­çº§  | æˆç»©  |
| ----- | ----- | ----- | ----- |
| <br/>  | <br/> | <br/> | <br/> |
| <br/> | <br/> | <br/> | <br/> |
| <br/> | <br/> | <br/> | <br/> |

 

<br><br><br><br>



 



<h5  align=center>å¤§è¿ç†å·¥å¤§å­¦è½¯ä»¶å­¦é™¢</h5>
<h5  align=center>2019å¹´7æœˆ</h5>








## é¡¹ç›®è¯´æ˜

ç¥ç­–é¢˜ç›®ï¼šâ€œç¥ç­–åˆ†æâ€ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æäº§å“ï¼ŒåŒ…å«ä¸€ä¸ªå®Œæ•´çš„æ•°æ®ä»“åº“ã€‚æ•°æ®ä»“åº“çš„å»ºè®¾æ˜¯æ•°æ®è¿›ä¸€æ­¥åº”ç”¨çš„åŸºç¡€ã€‚è€Œä¸€ä¸ªå®Œæ•´çš„æ•°æ®ä»“åº“é€šå¸¸æœ‰å¦‚ä¸‹æ¨¡å—ï¼šæ•°æ®é‡‡é›†ï¼ˆSDKã€å¯¼å…¥å·¥å…·ã€LogAgent ç­‰ï¼‰ã€æ•°æ®å¯¼å…¥ï¼ˆæ¸…æ´—ã€å…¥åº“ï¼‰ã€å­˜å‚¨ã€æŸ¥è¯¢å¼•æ“ã€åˆ†ææ¨¡å‹æŠ½è±¡å±‚ã€æ¥å£å±‚ã€UI äº¤äº’å±‚ã€‚å…¶ä¸­ä» â€æ•°æ®é‡‡é›†â€œ åˆ° â€å¯¼å…¥å­˜å‚¨â€œï¼Œæ¶‰åŠçš„æŠ€æœ¯ç»†èŠ‚éå¸¸ç¹æ‚ï¼Œå´åˆä¸èƒ½ç›´è§‚åœ°å›ç­”åˆå­¦è€…çš„ç–‘é—®ï¼šâ€œå…·ä½“ä¸šåŠ¡åœºæ™¯ä¸‹ï¼Œæ•°æ®åˆ†æåˆ°åº•æ˜¯å¦‚ä½•åº”ç”¨çš„â€œ è¿™ä¸ªé—®é¢˜ã€‚ä½†æŸ¥è¯¢å±‚ï¼ˆåŒ…æ‹¬æ¨¡å‹æŠ½è±¡ã€æŸ¥è¯¢å¼•æ“ï¼‰åˆ™ä¸åŒï¼ŒæŸ¥è¯¢ä¸€ä¾§æ˜¯è´´è¿‘ä¸šåŠ¡åº”ç”¨çš„ä¸€ä¾§ï¼Œç›¸å¯¹è€Œè¨€ä¼šæ›´åŠ ç›´è§‚ã€‚æ‰€ä»¥æœ¬æ¬¡çš„é¡¹ç›®æˆ‘ä»¬å°±ä»¥æŸ¥è¯¢å±‚ä¸ºåˆ‡å…¥ç‚¹ï¼Œå»å°è¯•æ­å»ºä¸€ä¸ªå¤§è§„æ¨¡ç”¨æˆ·è¡Œä¸ºåˆ†æç³»ç»Ÿã€‚

## æˆå‘˜åˆ†å·¥

| æˆå‘˜   | å­¦å·      | åˆ†å·¥                                                         |
| ------ | --------- | ------------------------------------------------------------ |
| å¾è¯—ç‘¶ | 201692126 | ä¸šåŠ¡é€»è¾‘è®¾è®¡+Impalaå®ç°+æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ+æ€»ä½“è®¾è®¡+åŠŸèƒ½å®ç°+ä¼˜åŒ–æ–¹æ³•å®ç° |
| ç‹è´   | 201672048 | ä¸šåŠ¡é€»è¾‘è®¾è®¡+ç•Œé¢è®¾è®¡+ç•Œé¢å®ç°+ç¯å¢ƒæ­å»º(Haoop+Hive+Impala)+æ•°æ®å¯¼å…¥+ä¼˜åŒ–æ–¹æ³• |
| å´ä»»é«˜ | 201671905 | æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ+æ€§èƒ½æµ‹è¯•+æ­£ç¡®æ€§æµ‹è¯•+ç¼–å†™ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ            |

## ç¯å¢ƒå®‰è£…

### 1.Hadoop 

##### ç¯å¢ƒå‡†å¤‡

##### jdkä¸‹è½½

åˆ°å®˜ç½‘ä¸‹è½½äº†jdk8 jdk-8u191-macosx-x64.dmgå®‰è£…jdk ä¹‹åé…ç½®ç¯å¢ƒå˜é‡å¦‚ä¸‹ï¼š
```
JAVA_HOME="Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home"
export JAVA_HOME
CLASS_PATH="$JAVA_HOME/lib"
PATH=".$PATH:$JAVA_HOME/bin"
export PATH="$HOME/.yarn/bin:$PATH"
```

> æ ¹æ®[è¿™ä¸ªæ•™ç¨‹](https://zhuanlan.zhihu.com/p/31162356)è£…å¥½äº†java

##### sshé…ç½®

å…ˆæŠŠç³»ç»Ÿåå¥½è®¾ç½®-å…±äº«-è¿œç¨‹ç™»å½•æ‰“å¼€
```
ssh localhost
```
æ˜¾ç¤ºéœ€è¦å¯†ç ï¼Œå®é™…ä¸Šå°±æ˜¯æœ¬æœºå¯†ç ï¼Œè¿™æ ·ä¸æ˜¯å¾ˆokï¼ˆå…·ä½“åˆ°åº•å“ªé‡Œä¸okæˆ‘ä¹Ÿä¸æ˜¯å¾ˆæ¸…æ¥š

terminalä¸­ä¿®æ”¹sshè®¾ç½®
```
ssh-keygen -t rsa
[è¿™é‡Œæœ‰å•¥è¾“å…¥çš„ä¸œè¥¿åæ­£æˆ‘ä»¬å°±æŒ‰å›è½¦å°±å®Œäº‹]
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod og-wx ~/.ssh.authorized_keys
```
è¿™æ—¶å€™æˆ‘ä»¬å†æ‰§è¡Œ
```
ssh localhost
```
å°±ä¼šå‘ç°ä¸éœ€è¦å¯†ç sshç™»é™†äº†ï½å°±å¯ä»¥ä¸‹è½½Hadoopäº†å‘¢ï¼

##### Hadoopä¸‹è½½å®‰è£…

###### å®˜ç½‘ä¸‹è½½

[å®˜ç½‘æä¾›çš„ä¸‹è½½åœ°å€](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.8.5/hadoop-2.8.5.tar.gz)æˆ‘ä¸‹è½½äº†2.8.5

ä¸‹è½½å®Œä¹‹åæˆ‘æŠŠè¿™ä¸ªtar.gzæ”¾åˆ°äº†/Documents/Hadoop æ–‡ä»¶å¤¹é‡Œ 
```
cd Hadoop
tar -zxvf hadoop-2.8.5.tar.gz
```

##### æ·»åŠ Hadoopç¯å¢ƒå˜é‡

åœ¨~/.bash_profileä¸­æ·»åŠ 
```
# Setting path for Hadoop
HADOOP_HOME="/Users/xusy/Documents/Hadoop/hadoop-2.8.5"
export HADOOP_HOME
export PATH=$PATH:HADOOP_HOME/sbin:$HADOOP_HOME/bin

export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native/
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native:$HADOOP_COMMON_LIB_NATIVE_DIR"
```
å…·ä½“è·¯å¾„æ ¹æ®hadoopçš„å®‰è£…ç›®å½•å†³å®š

ä¸‹åŠéƒ¨åˆ†çš„é…ç½®å¯ä»¥åœ¨ä¸Šé¢æåˆ°çš„ä¸€äº›

æ¥ä¸‹æ¥å¯ä»¥è¿›å…¥åˆ°æˆ‘ä»¬çš„Hadoopç›®å½•é‡Œ:

/hadoop-2.8.5/etc/hadoop/

ç„¶åä¿®æ”¹core-site.xml, mapred-site.xml(è¿™é‡Œæ˜¯mapred-site.xml.templateä¿®æ”¹æˆ.xml)

###### hadoop-env.sh

è¿™ä¸ªé…ç½®æ–‡ä»¶ç½‘ä¸Šæ‰¾åˆ°çš„å¤§éƒ¨åˆ†æ•™ç¨‹éƒ½è¦ä¿®æ”¹..ä½†æ˜¯..æˆ‘çœ‹å®Œæˆ‘ä¸‹è½½å®Œä¹‹åæ‰“å¼€çš„é»˜è®¤é…ç½®æ„Ÿè§‰ä¸ç”¨æ”¹..äºæ˜¯æ²¡æ”¹..

---æ›´æ–°---

åœ¨è¿™ä¸ªé…ç½®æ–‡ä»¶ä¸­åˆ æ‰äº†ä¸€äº›exportå‰çš„æ³¨é‡Š, å…³äºJAVA_HOME, JSVC_HOME, HADOOP_HOME, HADOOP_HEAPSIZE=1000(æˆ–è€…2000), HADOOP_OPTSä¸€äº›çš„æ³¨é‡Šéƒ½è¢«å»æ‰äº†ï¼Œæ— éœ€æ·»åŠ å•¥åˆ«çš„ä¸œè¥¿


---å†æ¥æ›´æ–°---

åœ¨åˆåˆåˆåˆå¯åŠ¨çš„æ—¶å€™å‘ç°è·‘ä»£ç çš„æ—¶å€™ä¼šæœ‰äº›é—®é¢˜..æŠ¥é”™ä¿¡æ¯æ˜¾ç¤ºçš„æ˜¯Javahomeçš„é—®é¢˜..ä»¥åŠHadoophomeçš„é—®é¢˜..å› æ­¤è¿˜æ˜¯å¯¹hadoop-env.shæ–‡ä»¶ä½œäº†ä¿®æ”¹ï¼Œå…·ä½“æ·»åŠ äº†javahome:
```
export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home"
export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"
export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS"

export HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_SECONDARYNAMENODE_OPTS"

export HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"
export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"
```
å…·ä½“çš„é…ç½®æ–‡ä»¶æ”¾åˆ°äº†æˆ‘çš„ GitHub -> HadoopClassNoteé‡Œï½


**åŒæ ·çš„ï¼š**åœ¨hadoop-env.sh, mapred-env.sh, yarn-env.shè¿™ä¸‰ä¸ªæ–‡ä»¶é‡Œéƒ½è¦å¯¹JAVA_HOMEè¿›è¡Œæ·»åŠ ä¿®æ”¹

###### core-site.xml

```
<configuration>
	<property>
		<name>fs.default.name</name>
		<value>hdfs://localhost:9000</value>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/Users/xusy/Documents/Hadoop</value>  ğŸ‘ˆğŸ¿æ˜¯è‡ªå®šä¹‰çš„æ”¾hdfsæ–‡ä»¶çš„ç›®å½•è¿™é‡Œæˆ‘å°±ç›´æ¥æ”¾åœ¨äº†æˆ‘çš„Hadoopç›®å½•é‡Œ
	</property>
</configuration>
```

(åæ¥ç”±äºnamenodeçš„ç›¸å…³ä¿¡æ¯å­˜åœ¨äº†ç³»ç»Ÿçš„tmpæ–‡ä»¶å¤¹é‡Œï¼Œå¯¼è‡´æ¯æ¬¡ç³»ç»Ÿé‡å¯çš„æ—¶å€™éƒ½ä¼šå‡ºç°é…ç½®ä¸èƒ½æˆåŠŸå¯åŠ¨ï¼Œæˆ‘ä»¬æ¯æ¬¡éƒ½è¦æ ¼å¼åŒ–namenodeï¼Œè¿™æ ·å°±éå¸¸ä¸okï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹è¿™ä¸ªæ–‡ä»¶ç¨å¾®ä¿®æ”¹äº†ä¸€ä¸‹)

```
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/Users/xusy/hadoop_tmp</value> 
	</property>
```

###### mapred-site.xml

è¿™ä¸ªæ–‡ä»¶å®é™…ä¸Šæˆ‘ä¸‹è½½å®Œçš„åç¼€æ˜¯.xml.template(è¿˜æ˜¯å•¥ç©æ„åæ­£æ˜¯åé¢æœ‰ä¸ªåç¼€ï¼Œè¢«æˆ‘ç›´æ¥ä¿®æ”¹æˆäº†.xml)
```
<configuration>
  <property>
    <name>mapred.job.tracker</name>
    <value>localhost:9010</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

###### hdfs-site.xml

```
<configuration>
	<!--ä¼ªåˆ†å¸ƒå¼-->
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
</configuration>
```
è¿™é‡Œçš„å˜é‡dfs.replicationæŒ‡å®šäº†æ¯ä¸ªHDFSæ•°æ®åº“çš„å¤åˆ¶æ¬¡æ•°ï¼Œé€šå¸¸ä¸º3ï¼Œè€Œæˆ‘ä»¬è¦åœ¨æœ¬æœºå»ºç«‹ä¸€ä¸ªä¼ªåˆ†å¸ƒå¼çš„DataNodeæ‰€ä»¥è¿™ä¸ªå€¼æ”¹æˆäº†1

ä¸ºäº†ä¿å­˜hdfsçš„å…ƒæ•°æ®å’Œdataç›¸å…³æ–‡ä»¶ï¼Œè¿™é‡Œåæ¥æ·»åŠ äº†propertyï¼š
```
<configuration>
	<!--ä¼ªåˆ†å¸ƒå¼-->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/Users/xusy/Documents/Hadoop/dfs/name</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/Users/xusy/Documents/Hadoop/dfs/data</value>
  </property>
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
  <property>
    <name>dfs.permissions</name>
    <value>false</value>
  </property>
</configuration>

```
###### yarn-site.xml

```
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>

<!-- Site specific YARN configuration properties -->

<!-- é›†ç¾¤é…ç½®-->
  <!--      <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>master</value>
      </property> -->

</configuration>
```
åŒæ ·çš„ç¨å¾®åšäº†ä¿®æ”¹
```
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>localhost:8031</value>
  </property>
    <property>
    <name>yarn.resourcemanager.address</name>
    <value>localhost:8032</value>
  </property>
    <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>localhost:8033</value>
  </property>
    <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>localhost:8034</value>
  </property>
    <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>localhost:8088</value>
  </property>
    <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
    <property>
    <name>yarn.log.server.url</name>
    <value>http://localhost:19888/jobhistory/logs/</value>
  </property>
<!-- Site specific YARN configuration properties -->

<!-- é›†ç¾¤é…ç½®-->
  <!--      <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>master</value>
      </property> -->
</configuration>
```

###### log4j.properties

åœ¨å…·ä½“è·‘ä»£ç çš„æ—¶å€™ä¼šæœ‰äº›WARNING(ä½†å®é™…ä¸Šä½ çš„ä»£ç å¹¶æ²¡æœ‰ä»€ä¹ˆé—®é¢˜..)å› æ­¤æˆ‘ä»¬è¦åœ¨log4j.propertiesæ–‡ä»¶åè¿½åŠ ä¸€è¡Œå†…å®¹ï¼š
```
log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR
```

##### å¯åŠ¨Hadoop

> æ¯æ¬¡æ“ä½œçš„æ—¶å€™éƒ½è¦è¿›å…¥è¿™ä¸ªHadoopæ–‡ä»¶å¤¹å“¦ï¼ˆå½“ç„¶æˆ‘è§‰å¾—å¦‚æœæŠŠè¿™ä¸ªæ·»åŠ åˆ°ç¯å¢ƒå˜é‡é‡Œä¼šä¸ä¼šå¥½ç‚¹..æˆ‘ä¹Ÿä¸çŸ¥é“æˆ‘çè¯´çš„

ç»ˆç«¯è¿›å…¥åˆ°Hadoopçš„æ–‡ä»¶å¤¹ä¸‹
æˆ‘è¿™é‡Œçš„æ–‡ä»¶å¤¹å°±æ˜¯
```
/Users/xusy/Documents/Hadoop/hadoop-2.8.5
```
æ‰§è¡Œ
```
./bin/hdfs namenode -format
```
æ ¼å¼åŒ–æ–‡ä»¶ç³»ç»Ÿï¼ˆå¯¹namenodeè¿›è¡Œåˆå§‹åŒ–)ï¼ˆå¥½åƒæ˜¯åªè¦åˆå§‹åŒ–ä¸€æ¬¡å°±å¥½äº†å°±æ˜¯æœ€å¼€å§‹å»ºç³»ç»Ÿçš„æ—¶å€™..ä¹‹åå¦‚æœæ¯æ¬¡å¯åŠ¨ä½ éƒ½åˆå§‹åŒ–..é‚£ä¹ˆæ˜¯ä¼šæœ‰é—®é¢˜çš„ï¼ï¼‰

---
æ›´æ–°

---

åœ¨å¯åŠ¨Hadoopï¼Œjpsä¹‹åå¯èƒ½ä¼šå‡ºç°ä½ çš„namenodeæ²¡èµ·æ¥çš„è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä¸ªæ—¶å€™å°±å¾—æ ¼å¼åŒ–ä¸€ä¸‹namenodeï¼Œå…·ä½“çš„è¯ğŸ‘‡ğŸ¿

è¿™é‡Œçš„namenode formatçš„é—®é¢˜ï¼šç”±äºnamenodeçš„ä¿¡æ¯æ˜¯å­˜åœ¨äº†ç³»ç»Ÿçš„tmpæ–‡ä»¶å¤¹ä¸‹çš„ï¼Œå¦‚æœä½ åˆ°è¿™é‡Œçœ‹çš„è¯æ˜¯èƒ½çœ‹è§è¿™äº›çš„ï¼š

æ¯æ¬¡å¯åŠ¨çš„è¯tmpæ˜¯ä¼šæ¸…ç©ºçš„ï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“å’‹å›äº‹åæ­£ï¼Œè™½ç„¶æˆ‘åœ¨core-site.xmlæ–‡ä»¶é‡Œæ˜æ˜å®šä¹‰çš„æ˜¯tmpå­˜åœ¨äº†Hadoopæ–‡ä»¶å¤¹ä¸‹...ä½†è¿˜æ˜¯æœ‰è¿™ä¸ªé—®é¢˜..æ‰€ä»¥å°±é‡æ–°åœ¨æˆ‘çš„xusyç”¨æˆ·ä¸‹é¢æ–°å»ºäº†ä¸€ä¸ªhadoop_tmpæ–‡ä»¶å¤¹ï¼ŒæŠŠä¸Šé¢core-site.xmlé‡Œå­˜tempçš„é‚£ä¸ªæ–‡ä»¶å¤¹è·¯å¾„æ”¹æˆäº†
```
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/Users/xusy/hadoop_tmp</value> 
```
ç„¶åé‡æ–°formatå°±å¯ä»¥äº†..ä¸çŸ¥é“å†é‡æ–°å¯åŠ¨æˆ‘çš„ç”µè„‘çš„æ—¶å€™è¿˜ä¼šä¸ä¼šæœ‰è¿™ä¸ªé—®é¢˜..å¦‚æœæœ‰é‚£å°±å†æ›´æ–°ä¸€ä¸‹..		


æ¥ä¸‹æ¥å¯åŠ¨namenode & datanode ï¼ˆæ„Ÿè§‰å°±æ˜¯å¯åŠ¨dfsæ–‡ä»¶ç³»ç»Ÿ)
```
./sbin/start-dfs.sh
```
ä¸­é—´ä¼šæœ‰ä¸€ä¸ªè¯¢é—®yes/noçš„æˆ‘ä»¬è¾“å…¥yeså°±å¥½äº†..
å¯åŠ¨yarn
```
./sbin/start-yarn.sh
```
å¯åŠ¨æ—¥å¿—ç®¡ç†logçš„histroyserver 
```
./mr-jobhistory-daemon.sh start historyserver
```
ğŸ‘†ğŸ¿è¾“å…¥äº†è¿™ä¸ªå‘½ä»¤å°±å¯ä»¥åœ¨jpsé‡Œçœ‹è§JobHistoryServeräº†

å½“ç„¶ä»¥ä¸Šçš„å‘½ä»¤éƒ½æ˜¯åœ¨hadoop-2.8.5ä¸‹é¢è¿è¡Œçš„

æƒ³è¦å…³é—­çš„è¯..
```
./sbin/stop-all.sh
# stop-dfs.sh stop-yarn.sh
```

æŸ¥çœ‹å½“å‰çš„hadoopè¿è¡Œæƒ…å†µ:
```
xushiyaodeMacBook-Pro:sbin xusy$ jps
39696 SecondaryNameNode
39809 ResourceManager
49810 JobHistoryServer
39891 NodeManager
39507 NameNode
69306 
39595 DataNode
73471 Jps
```
æµ‹è¯•ä¸€ä¸‹æˆ‘ä»¬èƒ½ä¸èƒ½è¿›å…¥åˆ°overviewç•Œé¢å‘¢ï¼

NameNode - http://localhost:50070

ps:è¿™é‡Œæœ‰ä¸€ä¸ªHadoop2å’ŒHadoop3å¯¹åº”ç«¯å£ä¿®æ”¹çš„è¡¨åœ¨ä¸‹é¢ï¼š

NameNodeç«¯å£

| Hadoop2 | Hadoop3 |
| ------: | ------: |
|   50470 |    9871 |
|   50070 |    9870 |
|    8020 |    9820 |

Secondary NNç«¯å£

| Hadoop2 | Hadoop3 |
| ------: | ------: |
|   50091 |    9869 |
|   50090 |    9868 |

DataNodeç«¯å£

| Hadoop2 | Hadoop3 |
| ------: | ------: |
|   50020 |    9867 |
|   50010 |    9866 |
|   50475 |    9865 |
|   50075 |    9864 |

##### ç»§ç»­å¯åŠ¨ï¼ï¼ï¼

ç”±äºæˆ‘ä»¬åˆšåˆšåˆ°é…ç½®..è¿™é‡Œçš„namenode1å¯¹åº”çš„å°±æ˜¯æˆ‘ä»¬æœ¬æœºlocalhostå•¦ï½(æ‰€ä»¥ä¸‹é¢çš„webæŸ¥çœ‹æ­£å¸¸è¾“å…¥çš„URLåº”è¯¥æ˜¯namenode1+ç«¯å£çš„)

overviewæŸ¥çœ‹ï¼

æŸ¥çœ‹HDFSï¼š

http://localhost:50070

æŸ¥çœ‹YARNï¼š

http://localhost:8088

æŸ¥çœ‹MRå¯åŠ¨JobHistory Server(è¿™é‡Œæš‚æ—¶å‡ºäº†é—®é¢˜..è®©æˆ‘ç ”ç©¶ä¸€ä¸‹..)

http://localhost:19888

### 2. Hive

ä¸€ã€å®‰è£… **MySQL**

1. ä¸Šä¼ MySQLåœ¨çº¿å®‰è£…æºçš„é…ç½®æ–‡ä»¶

ç”¨WinSCPï¼ˆrootè´¦å·è¿æ¥ï¼‰CentOSæœåŠ¡å™¨

å°†mysql-community.repo æ–‡ä»¶ä¸Šä¼ åˆ° /etc/yum.repos.d/ ç›®å½•

å°†RPM-GPG-KEY-mysql æ–‡ä»¶ä¸Šä¼ åˆ° /etc/pki/rpm-gpg/ ç›®å½•

 

2. æ›´æ–°yumæºå¹¶å®‰è£…mysql serverï¼ˆé»˜è®¤åŒæ—¶ä¼šå®‰è£…mysql clientï¼‰

> yum repolist

> yum install mysql-server

 

3. æŸ¥çœ‹MySQLå„ç»„ä»¶æ˜¯å¦æˆåŠŸå®‰è£…

> rpm -qa | grep mysql

![img](http://ww1.sinaimg.cn/large/006tNc79ly1g4lo4au6h4j308j01ngm8.jpg) 

 

 

äºŒã€é…ç½®**MySQL**

1. å¯åŠ¨MySQL Serverå¹¶æŸ¥çœ‹å…¶çŠ¶æ€

> systemctl start mysqld

> systemctl status mysqld

![img](http://ww4.sinaimg.cn/large/006tNc79ly1g4lo815xcsj30dz028gmn.jpg)ã€

2. æŸ¥çœ‹MySQLç‰ˆæœ¬

> mysql -V

![img](http://ww2.sinaimg.cn/large/006tNc79ly1g4lo83tyhbj30dz00mweq.jpg) 

 

3. è¿æ¥MySQLï¼Œé»˜è®¤rootå¯†ç ä¸ºç©º

> mysql -u root   (è¿™ä¸ªå‘½ä»¤ä¸å¥½ç”¨ï¼Œç”¨ mysql -u root -p )

> mysql> s

è¿™é‡Œå¦‚æœä½¿ç”¨ > myswl -u root ä¼šæŠ¥ä»¥ä¸‹é”™è¯¯

> ERROR 1044 (42000): Access denied for user ''@'localhost' to database 'mysql' 

4. æŸ¥çœ‹æ•°æ®åº“

> mysql> show databases; ï¼ˆæ³¨æ„ï¼šå¿…é¡»ä»¥åˆ†å·ç»“å°¾ï¼Œå¦åˆ™ä¼šå‡ºç°ç»­è¡Œè¾“å…¥ç¬¦â€œ>â€ï¼‰

 

5. åˆ›å»ºhiveå…ƒæ•°æ®æ•°æ®åº“ï¼ˆmetastoreï¼‰

> mysql> create database hive; 

![img](http://ww4.sinaimg.cn/large/006tNc79ly1g4lo9qcxrpj308o04z0tu.jpg) 

 

6. åˆ›å»ºç”¨æˆ·hiveï¼Œå¯†ç æ˜¯123456

> mysql> CREATE USER 'hive'@'%' IDENTIFIED BY '123456';

æ³¨æ„ï¼šåˆ é™¤ç”¨æˆ·æ˜¯DROP USERå‘½ä»¤ 

 

7. æˆæƒç”¨æˆ·hadoopæ‹¥æœ‰æ•°æ®åº“hiveçš„æ‰€æœ‰æƒé™

mysql> GRANT ALL PRIVILEGES ON hive.* TO 'hive'@'%' WITH GRANT OPTION;

 

8. æŸ¥çœ‹æ–°å»ºçš„MySQLç”¨æˆ·ï¼ˆæ•°æ®åº“åï¼šmysqlï¼Œè¡¨åï¼šuserï¼‰

> mysql> select host,user,password from mysql.user;

![img](http://ww1.sinaimg.cn/large/006tNc79ly1g4lo9tbz18j30dz04imyu.jpg) 

 

9. åˆ é™¤ç©ºç”¨æˆ·è®°å½•ï¼Œå¦‚æœæ²¡åšè¿™ä¸€æ­¥ï¼Œæ–°å»ºçš„hiveç”¨æˆ·å°†æ— æ³•ç™»å½•ï¼Œåç»­æ— æ³•å¯åŠ¨hiveå®¢æˆ·ç«¯

> mysql> delete from mysql.user where user='';

 

10. åˆ·æ–°ç³»ç»Ÿæˆæƒè¡¨ï¼ˆä¸ç”¨é‡å¯mysqlæœåŠ¡ï¼‰

> mysql> flush privileges; 

 

11. æµ‹è¯•hiveç”¨æˆ·ç™»å½•

> mysql -u hive -p

> Enter passwordï¼š123456



**ä¸‰ã€å®‰è£…å’Œé…ç½®hive**

1. ä¸‹è½½hive

> Wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz

2. è§£å‹hive-1.1.0-cdh5.12.1.tar.gzåˆ°/home/hadoop

> $ tar zxvf apache-hive-2.3.5-bin.tar.gz

 

3. åœ¨.bash_profileæ–‡ä»¶ä¸­æ·»åŠ hiveç¯å¢ƒå˜é‡

> export HIVE_HOME=/home/hadoop/hive-1.1.0-cdh5.12.1

> export PATH=$HIVE_HOME/bin:$PATH

4. ä½¿ä¸Šè¿°è®¾ç½®ç”Ÿæ•ˆ

   > $ source .bash_profile

 

5. ç¼–è¾‘$HIVE_HOME/conf/hive-env.shæ–‡ä»¶ï¼Œåœ¨æœ«å°¾æ·»åŠ HADOOP_HOMEå˜é‡

> cd $HIVE_HOME/conf

> cp hive-env.sh.template hive-env.sh	ï¼ˆé»˜è®¤ä¸å­˜åœ¨ï¼Œå¯ä»æ¨¡æ¿æ–‡ä»¶å¤åˆ¶ï¼‰

> vi hive-env.sh

> HADOOP_HOME=/root/Hadoop/hadoop-2.8.5

 

6. æ–°å»º$HIVE_HOME/conf/hive-site.xmlæ–‡ä»¶

```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://localhost:3306/hive</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>hive</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>123456</value>
        </property>

		<property>
				<name>hive.metastore.warehouse.dir</name>
				<value>/hive/warehouse</value>
		</property>
		<property>
				<name>hive.exec.scratchdir</name>
				<value>/hive/tmp </value>
		</property>
        <property>
                <name>hive.metastore.schema.verification</name>
                <value>false</value>
        </property>
</configuration>
```



 

7. åœ¨HDFSä¸Šåˆ›å»ºæ•°æ®ä»“åº“ç›®å½•ï¼ˆç”¨äºå­˜æ”¾hiveæ•°æ®æ–‡ä»¶ï¼‰å’Œä¸´æ—¶ç›®å½•

> hdfs dfs -mkdir -p /hive/warehouse /hive/tmp

 

8. ä¸‹è½½mysqlè¿æ¥é©±åŠ¨ï¼Œä¸‹è½½åœ°å€ï¼šhttps://dev.mysql.com/downloads/connector/j/

![img](file:////var/folders/nm/nfxnvn057nq5rsjjdhz11rsw0000gn/T/com.kingsoft.wpsoffice.mac/wps-bellick/ksohtml/wpsJYMEDH.png) 

	ä¸‹è½½æ–‡ä»¶(.tar.gz)è§£å‹åï¼Œå°†å…¶ä¸­çš„mysql-connector-java-8.0.13.jaræ–‡ä»¶ä¸Šä¼ åˆ° $HIVE_HOME/libç›®å½•ä¸‹

 



9. å¯åŠ¨hive

> hive

10. æŸ¥çœ‹hiveæ•°æ®åº“ ï¼ˆæ³¨æ„ï¼šå‘½ä»¤ä»¥åˆ†å·ç»“å°¾ï¼‰

> hive> show databases;

![img](file:////var/folders/nm/nfxnvn057nq5rsjjdhz11rsw0000gn/T/com.kingsoft.wpsoffice.mac/wps-bellick/ksohtml/wpsY2lKug.jpg) 

defaultæ˜¯é»˜è®¤æ•°æ®åº“

11. é€€å‡ºhive

> hive> quit;

##### Hive> Show databases; æŠ¥é”™

> hive> show databases;
> FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If needed, don't forget to include the option to auto-create the underlying database in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql))



åœ¨HIVE_HOME/conf/hive-site.xml ä¸­æ·»åŠ å¦‚ä¸‹é…ç½®

```
<property>
<name>datanucleus.schema.autoCreateAll</name>
<value>true</value>
</property>
```

### 3. Impala 

1. å…ˆå»http://archive.cloudera.com/beta/impala-kudu/redhat/7/x86_64/impala-kudu/0/RPMS/x86_64/ä¸‹è½½æ‰€éœ€çš„åŒ…
2. ä¾æ¬¡å®‰è£…è¿™äº›åŒ…

```shell
rpm -ivh bigtop-utils-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
rpm -ivh impala-xxx.rpm
```

3. impala é…ç½®

   3.1 æ·»åŠ hadoopå®‰è£…ç›®å½•ä¸‹çš„core-site.xml,hdfs.xml å’Œ hiveçš„hive-site.xml åˆ°/etc/impala/conf 

   3.2 ä¿®æ”¹æ–‡ä»¶ /etc/default/bigtop-utils ï¼Œæ–°å¢java_homeè·¯å¾„ï¼›

   3.3 ä¿®æ”¹æ–‡ä»¶ /etc/default/impalaï¼Œåªéœ€ä¿®æ”¹å‰ä¸¤è¡Œï¼Œæ”¹ä¸ºä¸»èŠ‚ç‚¹çš„ipåœ°å€æˆ–è€…hostname, è‹¥/etc/hostsæ–‡ä»¶é…ç½®äº† 127.0.0.1 localhost ï¼Œä¹Ÿå¯ä¸åšä¿®æ”¹

   3.4 ä¿®æ”¹core-site.xmlï¼Œæ–°å¢ä»¥ä¸‹å‡ é¡¹:

   ```
   <property>
           <name>dfs.client.read.shortcircuit</name>
           <value>true</value>
   </property>
   <property>
           <name>dfs.client.read.shortcircuit.skip.checksum</name>
           <value>false</value>
   </property>
   <property>
           <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
           <value>true</value>
   </property>
   ```

   3.5 ä¿®æ”¹hdfs-site.xmlï¼Œæ–°å¢ä»¥ä¸‹å‡ é¡¹:

   ```xml
   <property>
           <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
           <value>true</value>
   </property>
   <property>
           <name>dfs.block.local-path-access.user</name>
           <value>impala</value>
   </property>
   <property>
           <name>dfs.client.file-block-storage-locations.timeout.millis</name>
           <value>60000</value>
   </property>
   ```

   3.6 æƒé™é…ç½®

   > 1. sermod -G hdfs,hadoop impala
   > 2. groups impala

   3.7 åˆ›å»ºimpalaåœ¨hdfsç›®å½•ï¼Œèµ‹äºˆæƒé™(å•èŠ‚ç‚¹å³å¯)ï¼š

   > 1.  hdfs dfs -mkdir /user/impala
   > 2.  hadoop fs -chown impala /user/impala*

4. å¯åŠ¨impala ä¹‹å‰ï¼Œå…ˆå¯åŠ¨hadoop ,hiveserver2çš„æœåŠ¡(è‹¥é…ç½®äº†ï¼Œå¦åˆ™å¯åŠ¨hiveserveræœåŠ¡)

5. å¯åŠ¨impalaæœåŠ¡,  ä¸»æœºèŠ‚ç‚¹å³å¯ï¼Œä»æœºå¯ä»¥ä¸å¯åŠ¨impala-serveræœåŠ¡,æ‰€ç¤ºçš„ipä¸ºåˆšæ‰é…ç½®æ–‡ä»¶æ‰€é…çš„ipæˆ–è€…ä¸ºipå¯¹åº”çš„ hostnameï¼Œæœªä¿®æ”¹åˆ™ä¸º127.0.0.1ï¼š

   ```shell
   [root@master run]# service impala-state-store restart --kudu_master_hosts=192.168.174.132:7051
   Stopped Impala State Store Server:                         [  ç¡®å®š  ]
   Started Impala State Store Server (statestored):           [  ç¡®å®š  ]
   [root@master run]# service impala-catalog restart --kudu_master_hosts=192.168.174.132:7051
   Stopped Impala Catalog Server:                             [  ç¡®å®š  ]
   Started Impala Catalog Server (catalogd) :                 [  ç¡®å®š  ]
   [root@master run]# service impala-server restart --kudu_master_hosts=192.168.174.132:7051
   Stopped Impala Server:                                     [  ç¡®å®š  ]
   Started Impala Server (impalad):                           [  ç¡®å®š  ]
   [root@master run]# 
   ```

6. å¯åŠ¨ impala-shell

> åŸºäº https://blog.csdn.net/qq_41792743/article/details/87979146

## éœ€æ±‚åˆ†æ

1. äº‹ä»¶åˆ†æ

   * ç”¨æˆ·åœ¨äº§å“ä¸Šçš„è¡Œä¸ºæˆ‘ä»¬å®šä¹‰ä¸ºäº‹ä»¶ï¼Œå®ƒæ˜¯ç”¨æˆ·è¡Œä¸ºçš„ä¸€ä¸ªä¸“ä¸šæè¿°ï¼Œç”¨æˆ·åœ¨äº§å“ä¸Šçš„æ‰€æœ‰è·å¾—çš„ç¨‹åºåé¦ˆéƒ½å¯ä»¥æŠ½è±¡ä¸ºäº‹ä»¶è¿›è¡Œé‡‡é›†ã€‚äº‹ä»¶å¯ä»¥é€šè¿‡åŸ‹ç‚¹ã€é€šè¿‡å¯è§†åŒ–åœˆé€‰ç”Ÿæ•ˆï¼Œæ­¤æ–‡æ¡£ä»¥åŸ‹ç‚¹é‡‡é›†ä¸ºä¸»ã€‚å½“ç„¶ï¼Œä½ å¯ä»¥è‡ªå®šä¹‰äº‹ä»¶çš„åç§°ã€å±æ€§çš„åç§°ä»¥åŠä¸ªæ•°
   * åˆ†æå•ä¸ªäº‹ä»¶éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿ã€‚
   * æ ¹æ®äº‹ä»¶çš„æŸä¸ªæŒ‡æ ‡è§‚å¯Ÿå˜åŒ–è¶‹åŠ¿
   * æ ¹æ®ç”¨æˆ·å±æ€§æˆ–äº‹ä»¶å±æ€§è¿›è¡Œ**åˆ†ç»„å¯¹æ¯”**ï¼›

  åŸºäºä»¥ä¸Šæˆ‘ä»¬å¾—åˆ°äº‹ä»¶åˆ†æçš„åˆ†æç›®æ ‡ï¼š

  - å¯¹ä¸€ä¸ªæŒ‡æ ‡è¿›è¡Œåˆ†æï¼Œå¦‚â€æ”¯ä»˜è®¢å•â€œçš„â€è§¦å‘ç”¨æˆ·æ•°â€ï¼Œ
  - åˆ†ææŒ‡æ ‡å¯åŒ…æ‹¬â€œæ€»æ¬¡æ•°â€ã€â€œè§¦å‘ç”¨æˆ·æ•°â€ã€â€œäººå‡æ¬¡æ•°â€œã€â€å»é‡ç”¨æˆ·æ•°â€œ
  - ç”¨æˆ·å¯è‡ªè¡Œé€‰æ‹©äº‹ä»¶è¿›è¡Œåˆ†æï¼Œå¦‚ï¼šæ”¯ä»˜è®¢å•çš„è§¦å‘ç”¨æˆ·æ•°è¿™ä¸€äº‹ä»¶
  - ç”¨æˆ·å¯æŒ‰åˆ†ç»„/ç»´åº¦æŸ¥çœ‹åˆ†ææŒ‡æ ‡ï¼Œå¦‚æŒ‰å¹¿å‘Šæ¥æºåˆ†ç»„æŸ¥çœ‹æ”¯ä»˜è®¢å•çš„ç”¨æˆ·æ•°
  - ç”¨æˆ·å¯é€‰æ‹©ä¸åŒæ—¶é—´èŒƒå›´è¿›è¡ŒæŸ¥çœ‹

  åŸºäºæˆ‘ä»¬çš„åˆ†æç›®æ ‡ï¼ŒåŠç¥ç­–å®˜ç½‘çš„ä½¿ç”¨æ‰‹å†Œï¼Œæˆ‘ä»¬è®¾è®¡äº†åŒ…æ‹¬ä»¥ä¸ŠåŠŸèƒ½çš„äº‹ä»¶åˆ†æç•Œé¢ï¼Œå…·ä½“å‚è§ç•Œé¢è®¾è®¡éƒ¨åˆ†ã€‚

2. æ¼æ–—åˆ†æ

   * æ¼æ–—æ¨¡å‹ä¸»è¦ç”¨äºåˆ†æä¸€ä¸ªå¤šæ­¥éª¤è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„è½¬åŒ–ä¸æµå¤±æƒ…å†µã€‚

   * é€‰æ‹©éœ€è¦åˆ†æçš„æ—¥æœŸ

     ç”¨æˆ·å¯ä»¥é€‰æ‹©éœ€è¦åˆ†æçš„èµ·å§‹æ—¶é—´

   * ç‚¹å‡»åˆ›å»ºæ¼æ–—

     ç”¨æˆ·å¯ä»¥è‡ªå·±é€‰æ‹©åˆ›å»ºè‹¥å¹²æ¼æ–—è¿‡ç¨‹ã€‚

   * æ¼æ–—å›¾å±•ç¤º

     ç”¨æˆ·é€‰æ‹©æ—¶é—´å’Œæ¼æ–—åç‚¹å‡»æäº¤ï¼Œç³»ç»Ÿä¼šä¸ºç”¨æˆ·ç”»å‡ºæ¼æ–—å›¾ï¼Œå›¾ä¸­æ ‡è®°å‡ºæ¯ä¸ªè¿‡ç¨‹çš„ç”¨æˆ·æ•°ï¼Œç›¸é‚»æ¼æ–—çš„é¢ç§¯å¯¹æ¯”å³æ˜¯è¯¥è¿‡ç¨‹çš„è½¬åŒ–ç‡ã€‚

3. ç•™å­˜åˆ†æ

   * ç”¨æˆ·é€‰æ‹©åˆ†æçš„æ—¶é—´æ®µ

     ç”¨æˆ·å¯ä»¥è‡ªä¸»é€‰æ‹©åˆ†æçš„èµ·æ­¢æ—¶é—´ï¼Œç²’åº¦ä¸ºæ—¥

   * ç”¨æˆ·é€‰æ‹©åˆå§‹è¡Œä¸º

     åˆå§‹è¡Œä¸ºé€‰æ‹©ç”¨æˆ·åªè§¦å‘ä¸€æ¬¡çš„äº‹ä»¶ï¼Œæ¯”å¦‚â€œæ³¨å†Œâ€ã€â€œä¸Šä¼ å¤´åƒâ€ã€â€œæ¿€æ´»è®¾å¤‡â€ç­‰ã€‚

   * ç”¨æˆ·é€‰æ‹©åç»­è¡Œä¸º

     åç»­è¡Œä¸ºé€‰æ‹©ä½ æœŸæœ›ç”¨æˆ·é‡å¤è§¦å‘çš„äº‹ä»¶ï¼Œæ¯”å¦‚â€œé˜…è¯»æ–‡ç« â€ã€â€œå‘å¸–â€ã€â€œè´­ä¹°â€ç­‰ã€‚è¿™ç§ç•™å­˜ç”¨äºå¯¹æ¯”åˆ†æä¸åŒé˜¶æ®µå¼€å§‹ä½¿ç”¨äº§å“çš„æ–°ç”¨æˆ·çš„å‚ä¸æƒ…å†µï¼Œä»è€Œè¯„ä¼°äº§å“è¿­ä»£æˆ–è¿è¥ç­–ç•¥è°ƒæ•´çš„å¾—å¤±ã€‚

4. åŠŸèƒ½å±•ç¤º

   * ç”¨æˆ·é€šè¿‡ç½‘é¡µè¡¨å•é€‰æ‹©åŠŸèƒ½éœ€æ±‚
   * åç«¯æ¥æ”¶ç½‘é¡µä¼ æ¥çš„æ•°æ®

5. éœ€æ±‚å½’çº¦

6. æ•°æ®å­—å…¸

## æ•°æ®å¯¼å…¥

å°†æ•°æ®æ–‡ä»¶æ‹·è´åˆ°HDFSä¸Šï¼Œç„¶åå»ºç«‹ä¸€å¼ impalaå¤–éƒ¨è¡¨ï¼Œå°†å¤–éƒ¨è¡¨çš„å­˜å‚¨ä½ç½®ï¼ŒæŒ‡å‘æ•°æ®æ–‡ä»¶

![IMG_996067991597-1](http://ww1.sinaimg.cn/large/006tNc79ly1g53ojhwe48j31kw0r74qp.jpg)

1. ç”¨scpå°†æ•°æ®æ–‡ä»¶ä¼ åˆ°æœåŠ¡å™¨

2. åœ¨HDFSä¸Šå»ºç«‹å­˜å‚¨æ•°æ®çš„ç›®å½•

   > su hdfs
   >
   > hdfs dfs -mkdir -p  /user/impala/data /user/impala/data/event_export /user/impala/data/user_export

3. ä¿®æ”¹HDFSç›®å½•æƒé™ï¼ˆå¦‚æœéœ€è¦ï¼‰

   > hdfs dfs -chmod 777 /user/impala/data/event_export 

4. å°†æ•°æ®æ–‡ä»¶ä¼ åˆ°HDFSæŒ‡å®šç›®å½•ä¸Š

   > hdfs dfs -put /home/work/event_export/xxxxxx.xxx  /user/impala/data/event_export 

   >  hdfs dfs -put /home/work/user_export/xxxxxx.xxx  /user/impala/data/user_export 

5. åœ¨impala-shellä¸­å»ºç«‹å¤–éƒ¨è¡¨ï¼Œå¹¶æŒ‡å‘æ•°æ®æ–‡ä»¶

   > Impala-shell > CREATE TABLE rawdata.event_export (
   >   event_id INT,
   >   month_id INT,
   >   week_id INT,
   >   user_id BIGINT,
   >   distinct_id STRING,
   >   time TIMESTAMP,
   >   day INT,
   >   event_bucket INT,
   >   _offset BIGINT,
   >   p__app_version STRING,
   >   ...
   > )
   > STORED AS TEXTFILE
   > LOCATION '/user/impala/data/event_export '

## æ€»ä½“è®¾è®¡

#### 1. äº‹ä»¶åˆ†æ

1. ç”¨æˆ·é€‰æ‹©æ—¶é—´æ®µ 

2. ç”¨æˆ·é€‰æ‹©äº‹ä»¶ï¼ˆè¡Œä¸ºï¼‰-> äº‹ä»¶ä¸‹æ‹‰æ¡†

3. ç”¨æˆ·é€‰æ‹©äº‹ä»¶çš„å±•ç¤ºæŒ‡æ ‡ -> æŒ‡æ ‡ä¸‹æ‹‰æ¡†ï¼ˆ5ä¸ªæŒ‡æ ‡maxï¼‰-> æŒ‡æ ‡é€šè¿‡å­—å…¸æ˜ å°„åˆ°sql

4. ç”¨æˆ·é€‰æ‹©æŒ‰æŸç§æŒ‡æ ‡åˆ†ç»„

   3.1 å±•ç¤ºæŒ‡æ ‡ï¼šæ€»æ¬¡æ•°ã€æ€»äººæ•°ã€å»é‡äººæ•°ã€äººå‡æ¬¡æ•°ã€å¹³å‡äº‹ä»¶æ—¶é•¿ã€
   4.1 åˆ†ç»„æŒ‡æ ‡ï¼šå¹¿å‘Šç³»åˆ—æ¥æº -> æ¥æºåˆ†æå¯å¸®åŠ©ç”¨æˆ·è¿›è¡Œå¹¿å‘ŠæŠ•æ”¾ã€æ˜¯å¦é¦–æ¬¡è®¿é—®

#### 2. æ¼æ–—åˆ†æ

æ¼æ–—æµç¨‹ï¼š

ğŸŒ°ï¼šç‚¹å‡»å¿˜è®°å¯†ç id=5 -> æ‰¾å›å¯†ç -è·å–éªŒè¯ç id=19 -> æ‰¾å›å¯†ç -é‡ç½®å¯†ç id=28 -> æäº¤æ–°å¯†ç id=1

1. ç”¨æˆ·é€‰æ‹©éœ€è¦æŸ¥è¯¢è¿‡æ»¤çš„å¹´ï¼Œæœˆ
2. ç”¨æˆ·æŒ‰é¡ºåºé€‰æ‹©éœ€è¦è¿‡æ»¤çš„æµç¨‹ï¼ˆ4æ­¥ï¼‰
3. è¿”å›æœ¬æœˆä¸­å¯¹åº”æµç¨‹çš„äººæ•°å’Œè½¬åŒ–æ¯”ä¾‹

#### 3. ç•™å­˜åˆ†æ

1. ç”¨æˆ·é€‰æ‹©æ—¶é—´æ®µ

2. ç”¨æˆ·åˆå§‹è¡Œä¸º

3. ç”¨æˆ·é€‰æ‹©åç»­è¡Œä¸º

4. å±•ç¤ºæ—¶é—´æ®µå†…7å¤©ç•™å­˜çš„ç»“æœåˆ†æï¼šæ€»äººæ•°ï¼Œ1å¤©ä¹‹å†…æ¯”ä¾‹ï¼Œç¬¬äºŒå¤©æ¯”ä¾‹...ç¬¬ä¸ƒå¤©æ¯”ä¾‹

   4.1è¿”å›çš„ç»“æ„æ˜¯ä¸€å¼ ä»from_timeåˆ°to_timeè¿™ä¹ˆå¤šè¡Œï¼Œæ¯è¡Œå…ƒç´ æ˜¯æ€»äººæ•°ï¼Œ1å¤©ï¼Œ2å¤©...ç¬¬ä¸ƒå¤©æ¯”ä¾‹ è¿™ä¹ˆå¤šåˆ—çš„è¡¨

## ç•Œé¢è®¾è®¡

æŒ‰ç…§ç¥ç­–çš„æ–‡æ¡£ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªé˜‰å‰²ç‰ˆçš„ç•Œé¢

* å¯¹äºäº‹ä»¶åˆ†æï¼Œæˆ‘ä»¬å…è®¸ç”¨æˆ·é€‰æ‹©
  * äº‹ä»¶çš„æ—¶é—´åŒºé—´
  * åˆ†ææŒ‡æ ‡
  * åˆ†ç»„å±•ç¤ºæ–¹å¼

![](http://ww1.sinaimg.cn/large/006tNc79ly1g50gaf712xj30vq0a0mye.jpg)

* å¯¹äºç•™å­˜åˆ†æï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©
  * äº‹ä»¶èµ·æ­¢æ—¥æœŸ
  * ç”¨æˆ·åˆå§‹è¡Œä¸º
  * ç”¨æˆ·åç»­è¡Œä¸º

![](http://ww1.sinaimg.cn/large/006tNc79ly1g50gaqm0v0j30vq08i75d.jpg)

* å¯¹äºæ¼æ–—åˆ†æï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©
  * å¹´ä»½
  * æœˆä»½
  * æ„æˆæ¼æ–—çš„è¡Œä¸º X 4

![](http://ww2.sinaimg.cn/large/006tNc79ly1g50g9yya1gj30vq0fxdgv.jpg)

## åŠŸèƒ½å®ç°

æˆ‘ä»¬åŸºäºimpyla åŒ…å®ç°ç”¨Pythonè¿æ¥impalaï¼Œåœ¨Pythonä¸­ç¼–è¾‘impala-SQLè¯­å¥ï¼Œé€šè¿‡è¿œç¨‹æäº¤æŸ¥è¯¢è¯·æ±‚æ¥ä½¿impalaåšå‡ºå“åº”ã€‚

#### 1. äº‹ä»¶åˆ†æ

åŸºäºæˆ‘ä»¬çš„éœ€æ±‚ï¼Œæˆ‘ä»¬å°†ç”¨æˆ·å‰å°è¿”å›çš„from_time, to_time, event_id, feature, groupä¼ å…¥å‡½æ•°ä¸­è¿›è¡Œå¤„ç†ã€‚å„ä¸ªå‚æ•°å…·ä½“è§£é‡Šå¦‚ä¸‹ï¼š

`from_time:` ç”¨æˆ·é€‰æ‹©çš„èµ·å§‹æ—¶é—´

`to_time:` ç”¨æˆ·é€‰æ‹©çš„ç»“æŸæ—¶é—´

`event_id:` ç”¨æˆ·é€‰æ‹©è¦åˆ†æçš„æ—¶é—´

`feature:` ç”¨æˆ·é€‰çš„çš„è¦åˆ†æçš„æŒ‡æ ‡ï¼Œè€ƒè™‘åˆ°ç”¨æˆ·å¯èƒ½é€‰æ‹©åˆ†ææ€»äººæ•°ã€æ€»æ¬¡æ•°ã€å¹³å‡äº‹ä»¶æ—¶é•¿ã€äººå‡æ¬¡æ•°ã€å»é‡äººæ•°ç­‰ä¸åŒåˆ†ææŒ‡æ ‡ï¼Œæˆ‘ä»¬å°†è¿™äº›æŒ‡æ ‡å­˜åˆ°ä¸€ä¸ªå­—å…¸ä¸­è¿›è¡Œå¤„ç†ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

```python    
features = {
        "0": "",  # æ€»æ¬¡æ•°
        "1": "",  # æ€»äººæ•°
        "2": "",  # å»é‡äººæ•°
        "3": "",  # äººå‡æ¬¡æ•°
        # "4":"", # å¹³å‡äº‹ä»¶æ—¶é•¿
    }
```
`group:` ç”¨æˆ·é€‰æ‹©è¦åˆ†ç»„å±•ç¤ºçš„å†…å®¹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸featureä¸€æ ·çš„å¤„ç†åŠæ³•ï¼Œå°†ç”¨æˆ·å¯èƒ½é€‰æ‹©çš„åˆ†ç»„æƒ…å†µå¦‚ï¼šå¹¿å‘Šç³»åˆ—æ¥æºï¼ˆè¿è¥å•†ï¼‰ã€æ˜¯å¦é¦–æ¬¡è®¿é—®ï¼ˆåˆ¶é€ å•†ï¼‰å­˜å…¥å­—å…¸ä¸­æ–¹ä¾¿åç»­è¿›è¡Œå¤„ç†ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

```python
    groups = {
        "0": "",  # å¹¿å‘Šç³»åˆ—æ¥æºåˆ†ç»„->è¿è¥å•†
        "1": "",  # æ˜¯å¦é¦–æ¬¡è®¿é—®åˆ†ç»„->è®¾å¤‡åˆ¶é€ å•†
    }
```
åœ¨äº‹ä»¶åˆ†æä¸­ï¼Œåˆ†æçš„ä¸»è¦ç›®æ ‡æ˜¯è¦å°†ç”¨æˆ·é€‰æ‹©çš„æ—¶é—´æŒ‰ç…§é€‰æ‹©çš„featureè¿›è¡Œå±•ç¤ºï¼Œæ ¹æ®ç”¨æˆ·æ“ä½œæµç¨‹æˆ‘ä»¬æŒ‰ä¸€ä¸‹æµç¨‹è®¾è®¡å¹¶å®ç°äº†å‡½æ•°åŠŸèƒ½ã€‚

1. ç”¨æˆ·åœ¨é¡µé¢é€‰æ‹©
  
    * æ—¶é—´æ®µ( yyyy-mm-dd,yyyy-mmâ€”dd)ï¼Œå³æŸ¥è¯¢çš„èµ·å§‹æ—¥æœŸã€ç»ˆæ­¢æ—¥æœŸ

    * é€‰æ‹©æŸ¥è¯¢äº‹ä»¶: event_id

    * é€‰æ‹©åˆ†ææŒ‡æ ‡: feature 

    * é€‰æ‹©åˆ†ç»„å±•ç¤ºæ–¹å¼: group 

2. é¦–å…ˆå°†å‰ç«¯ä¼ å…¥çš„æ—¶é—´æ®µè½¬æ¢æˆUnixTimestampï¼Œåè€ƒè™‘åˆ°æˆ‘ä»¬æ ¹æ®dayè¿›è¡Œäº†partitionçš„ä¼˜åŒ–ï¼Œå†å°†UnixTimestampè½¬æˆdayï¼ŒåŠ å¿«æŸ¥è¯¢é€Ÿåº¦ã€‚
```python
    from_time += " 00:00:00"
    to_time += " 00:00:00"
    from_time = time.strptime(from_time, "%Y-%m-%d %H:%M:%S")
    from_day = str(int(time.mktime(from_time) // 86400))
    to_time = time.strptime(to_time, "%Y-%m-%d %H:%M:%S")
    to_day = str(int(time.mktime(to_time) // 86400))
```

3. ç„¶åç­›é€‰å‡ºç”¨æˆ·é€‰å®šæ—¶é—´æ®µå†…ï¼Œä¸ç”¨æˆ·é€‰å®šäº‹ä»¶event_idçš„æ•°æ®ï¼Œè¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼ŒåŒæ—¶åˆ›å»ºä¸€ä¸ªviewæ–¹ä¾¿åç»­æŸ¥è¯¢ã€‚
```python
    create_string = "create view sample_event as select * from event_export_partition where event_id=" + event_id + " and " + \
                    from_day + " <day and day< " + to_day

    cur.execute('use group7')
    cur.execute('drop view if exists group7.sample_event')
    cur.execute(create_string)
```

4. åœ¨åŠŸèƒ½è®¾è®¡æ—¶ï¼Œæˆ‘ä»¬çš„å‡½æ•°åŠŸèƒ½æ˜¯è®©ç”¨æˆ·å¯ä»¥è‡ªè¡Œé€‰æ‹©è¦åˆ†æçš„æŒ‡æ ‡ï¼Œå› æ­¤æ¥ä¸‹æ¥æˆ‘ä»¬è¦é’ˆå¯¹ä¸åŒçš„æŒ‡æ ‡ç¼–å†™ä¸åŒçš„sqlè¯­å¥ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥çš„featureä¸åŒï¼Œé‡‡ç”¨å­—å…¸çš„ç´¢å¼•æ–¹å¼é€‰æ‹©ä¸åŒçš„sqlè¯­å¥è¿›è¡Œæ‰§è¡Œï¼Œå…·ä½“å¦‚ä¸‹ã€‚
```python
    features = {
        "0": "",  # æ€»æ¬¡æ•°
        "1": "",  # æ€»äººæ•°
        "2": "",  # å»é‡äººæ•°
        "3": "",  # äººå‡æ¬¡æ•°
        # "4":"", # å¹³å‡äº‹ä»¶æ—¶é•¿
    }
    features["0"] = "select count(time),day from sample_event group by day order by day"
    features["1"] = "select count(user_id),day from sample_event group by day order by day"
    features["2"] = "select count(distinct user_id),day from sample_event group by day order by day"
    features["3"] = "select count(time)/count(distinct user_id),day from sample_event group by day order by day"
    # features["4"]="select sum(p__event_duration)/count(p__event_duration),day from sample_event group by day"

    f = {
        "0": "count(time)",
        "1": "count(user_id)",
        "2": "count(distinct user_id)",
        "3": "count(time)/count(distinct user_id)"
    }
    cur.execute(features[feature])
    feature_result = cur.fetchall()
    feature_result = [list(x) for x in feature_result]
```

5. å¯¹ç”¨æˆ·é€‰æ‹©çš„æŒ‰ä¸åŒåˆ†ç»„å±•ç¤ºçš„groupè¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸featureç±»ä¼¼çš„è®¾è®¡æ–¹å¼ï¼Œå³è®¾è®¡groupå­—å…¸ï¼Œå°†ç”¨æˆ·é€‰æ‹©çš„ä¸åŒgroupæ ¹æ®ç´¢å¼•æ˜ å°„åˆ°ä¸åŒçš„sqlè¯­å¥è¿›è¡Œæ‰§è¡Œã€‚è¿™é‡Œåªèµ·åˆ°ä¸€ä¸ªæ¼”ç¤ºä½œç”¨ï¼Œæˆ‘ä»¬åªå†™äº†ä¸¤ä¸ªgroupæŒ‡æ ‡ï¼Œåç»­å¯ä»¥æ ¹æ®æ•°æ®ç‰¹ç‚¹ï¼Œä¸åŒéœ€æ±‚è¿›è¡ŒæŒ‡æ ‡æ•°é‡çš„æ·»åŠ ã€‚
```python
    groups = {
        "0": "",  # å¹¿å‘Šç³»åˆ—æ¥æºåˆ†ç»„->è¿è¥å•†
        "1": "",  # æ˜¯å¦é¦–æ¬¡è®¿é—®åˆ†ç»„->è®¾å¤‡åˆ¶é€ å•†
    }
    groups["0"] = "select " + f[feature] + ",p__carrier,day from sample_event group by day,p__carrier order by day"
    groups["1"] = "select " + f[feature] + ",p__manufacturer,day from sample_event group by day,p__manufacturer order by day"

    cur.execute(groups[group])
    group_result = cur.fetchall()
    group_result = [list(x) for x in group_result]
```

6. æ€»çš„å‡½æ•°ï¼š
```python
def event(host,from_time, to_time, event_id, feature,
          group):  # from_time: "2019-01-01", event_id: str, feature: str, group: str
    conn = connect(host=host, port=21050)
    cur = conn.cursor()
    features = {
        "0": "",  # æ€»æ¬¡æ•°
        "1": "",  # æ€»äººæ•°
        "2": "",  # å»é‡äººæ•°
        "3": "",  # äººå‡æ¬¡æ•°
        # "4":"", # å¹³å‡äº‹ä»¶æ—¶é•¿
    }
    groups = {
        "0": "",  # å¹¿å‘Šç³»åˆ—æ¥æºåˆ†ç»„->è¿è¥å•†
        "1": "",  # æ˜¯å¦é¦–æ¬¡è®¿é—®åˆ†ç»„->è®¾å¤‡åˆ¶é€ å•†
    }
    from_time += " 00:00:00"
    to_time += " 00:00:00"
    from_time = time.strptime(from_time, "%Y-%m-%d %H:%M:%S")
    from_day = str(int(time.mktime(from_time) // 86400))
    to_time = time.strptime(to_time, "%Y-%m-%d %H:%M:%S")
    to_day = str(int(time.mktime(to_time) // 86400))

    create_string = "create view sample_event as select * from event_export_partition where event_id=" + event_id + " and " + \
                    from_day + " <day and day< " + to_day

    cur.execute('use group7')
    cur.execute('drop view if exists group7.sample_event')
    cur.execute(create_string)

    features["0"] = "select count(time),day from sample_event group by day order by day"
    features["1"] = "select count(user_id),day from sample_event group by day order by day"
    features["2"] = "select count(distinct user_id),day from sample_event group by day order by day"
    features["3"] = "select count(time)/count(distinct user_id),day from sample_event group by day order by day"
    # features["4"]="select sum(p__event_duration)/count(p__event_duration),day from sample_event group by day"

    f = {
        "0": "count(time)",
        "1": "count(user_id)",
        "2": "count(distinct user_id)",
        "3": "count(time)/count(distinct user_id)"
    }

    # groups["0"] = "select "+f[feature]+",p_utm_source,day from sample_event group by day,p_utm_source order by day"
    # groups["1"] = "select "+f[feature]+",p_is_first_time,day from sample_event group by day,p_is_first_time order by day"
    groups["0"] = "select " + f[feature] + ",p__carrier,day from sample_event group by day,p__carrier order by day"
    groups["1"] = "select " + f[feature] + ",p__manufacturer,day from sample_event group by day,p__manufacturer order by day"

    cur.execute(features[feature])
    feature_result = cur.fetchall()
    feature_result = [list(x) for x in feature_result]
    # for x in feature_result:
    #     x[1] = str(datetime.datetime.fromtimestamp(x[1] * 86400))[:10]

    cur.execute(groups[group])
    group_result = cur.fetchall()
    group_result = [list(x) for x in group_result]
    # for x in group_result:
    #     x[2] = str(datetime.datetime.fromtimestamp(x[2] * 86400))[:10]
    return feature_result, group_result

```

#### 2. æ¼æ–—åˆ†æ

åŸºäºæˆ‘ä»¬çš„è¦æ±‚ï¼Œç”¨æˆ·éœ€å…ˆé€‰æ‹©å¾…åˆ†æçš„æ—¶é—´ï¼Œä¹‹åå¯ä»¥é€‰æ‹©ä¸åŒçš„æ­¥éª¤è¿›è¡Œæ¼æ–—è¿‡æ»¤ã€‚æ ¹æ®å‰ç«¯è®¾è®¡çš„ç•Œé¢ï¼Œæœ¬é¡¹ç›®å…è®¸ç”¨æˆ·æ·»åŠ å››ä¸ªæ­¥éª¤è¿›è¡Œæ¼æ–—åˆ†æã€‚åŸºäºä»¥ä¸Šæµç¨‹ï¼Œéœ€ä¼ å…¥çš„æ•°æ®å¦‚ä¸‹ï¼ševent_ids,quaryï¼Œå…·ä½“è§£é‡Šå¦‚ä¸‹ï¼š

`event_ids:` å­˜å‚¨ç”¨æˆ·é€‰æ‹©çš„å››ä¸ªæ­¥éª¤çš„event_id

`quary:` åŒ…å«ç”¨æˆ·é€‰æ‹©çš„æ—¶é—´å­—æ®µï¼Œæ ¼å¼ä¸ºï¼š[year, month]

æ ¹æ®impalaä¸­å­˜å‚¨æ•°æ®çš„ç‰¹ç‚¹ï¼Œé¦–å…ˆè¦å°†quaryä¸­å­˜å‚¨çš„å¹´ï¼Œæœˆè¿›è¡Œå¤„ç†
```python
    # quaryå¤„ç†
    from_month = "'" + quary[0] + "-" + quary[1] + "-01 00:00:00.000000000'"
    if int(quary[1]) < 12:
        to_month = "'" + quary[0] + "-" + "{:0>2d}".format(int(quary[1]) + 1) + "-01 00:00:00.000000000'"
    else:
        to_month = "'" + str(int(quary[0]) + 1) + "-01-01 00:00:00.000000000'"
```
åœ¨è¿›è¡Œæ¼æ–—çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰çš„æ¼æ–—è¿‡æ»¤çš„æ—¶é•¿ä¸º2å°æ—¶ï¼Œå…·ä½“é€šè¿‡timecmpå‡½æ•°å®ç°ï¼Œå‚åŠ åç»­æµç¨‹ã€‚

1. ç”¨æˆ·åœ¨ç•Œé¢é€‰æ‹©

   * æ—¶é—´æ®µ quary ( yyyy-mm-dd,yyyy-mmâ€”dd) 

   * é€‰æ‹©çš„æ¼æ–—æµç¨‹ event_ids

2. é¦–å…ˆå¤„ç†ç”¨æˆ·é€‰æ‹©çš„æŸ¥è¯¢æ—¶é—´ï¼Œå°†å…¶è½¬æ¢æˆUnixTimestampï¼Œè€ƒè™‘åˆ°æ¼æ–—åˆ†æçš„è¿‡æ»¤æ—¶é—´ä¸º2ä¸ªå°æ—¶è‹¥å°†quaryè½¬æˆdayå®Œå…¨æ²¡æœ‰å¿…è¦ï¼Œæ¼æ–—åˆ†æä¸­æ²¡å¿…è¦æŒ‰ç…§dayè¿›è¡Œèšåˆæˆ–è¿›è¡ŒæŸ¥è¯¢ç­‰ï¼Œå› æ­¤å°†å…¶è½¬æˆtimeå³å¯ã€‚
å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š
```python
    # quaryå¤„ç†
    from_month = "'" + quary[0] + "-" + quary[1] + "-01 00:00:00.000000000'"
    if int(quary[1]) < 12:
        to_month = "'" + quary[0] + "-" + "{:0>2d}".format(int(quary[1]) + 1) + "-01 00:00:00.000000000'"
    else:
        to_month = "'" + str(int(quary[0]) + 1) + "-01-01 00:00:00.000000000'"
```

3. æ¥ä¸‹æ¥å¯¹å¾…æŸ¥è¯¢çš„è¡¨è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒæ ·çš„ç­›é€‰å‡ºç”¨æˆ·é€‰å®šæ—¶é—´æ®µå†…ï¼ŒåªåŒ…å«ç”¨æˆ·é€‰ä¸­çš„è¿‡æ»¤æµç¨‹event_idsçš„æ•°æ®ï¼Œæ–¹ä¾¿åç»­è¿›ä¸€æ­¥æŸ¥æ‰¾ã€‚

```python
    create_string = "create view sample_funnel as select user_id, event_id, time from event_export_partition where event_id in" + \
                    str(event_ids) + " and " + from_month + " <time and time< " + to_month
    cur.execute('use group7')
    cur.execute('drop view if exists group7.sample_funnel')
    cur.execute(create_string)
```

4. æ ¹æ®æ¼æ–—åˆ†æçš„ç‰¹ç‚¹ä¸æœ€ç»ˆåº”è¾“å‡ºç»“æœï¼Œæˆ‘ä»¬é¦–å…ˆç­›é€‰å‡ºç¬¬ä¸€æ­¥æµç¨‹ä¸­å‚ä¸äººæ•°ï¼Œè®°ä¸ºcount0,ä¹‹åé€šè¿‡join onï¼Œæ·»åŠ event_idçš„é™åˆ¶æ¡ä»¶è¡¨ç¤ºç”¨æˆ·é€‰æ‹©çš„æ¯ä¸ªæ­¥éª¤ï¼Œç­›é€‰å‡ºæ¯ä¸ªæ­¥éª¤ä¸­å®Œæˆä¸Šä¸€æ­¥éª¤çš„äººæ•°ï¼Œè®°ä¸ºcount1,count2,count3å…·ä½“å®ç°å¦‚ä¸‹ï¼š

```python 
    create_string = "select count(t1.time),count(t2.time), count(t3.time) from (select * from sample_funnel where event_id=" \
                    + str(event_ids[0]) + ") t0" + \
                    " left join (select * from sample_funnel where event_id=" + str(event_ids[1]) + ") t1" + \
                    " on t0.user_id=t1.user_id and t0.time<t1.time and timestamp_cmp(t0.time + interval 120 minutes, t1.time)=1" + \
                    " left join (select * from sample_funnel where event_id=" + str(event_ids[2]) + ") t2" + \
                    " on t1.user_id=t2.user_id and t1.time<t2.time and timestamp_cmp(t1.time + interval 120 minutes, t2.time)=1" + \
                    " left join (select * from sample_funnel where event_id=" + str(event_ids[3]) + ") t3" + \
                    " on t2.user_id=t3.user_id and t2.time<t3.time and timestamp_cmp(t2.time + interval 120 minutes, t3.time)=1"
    cur.execute(create_string)
    data = cur.fetchall()
    count1, count2, count3 = data[0][0], data[0][1], data[0][2]
```

5. æ€»çš„å‡½æ•°

```python
def funnel(host,event_ids, quary):  # event_ids->tuple; quary->[year,month] # æŒ‰æœˆä»½è¿›è¡Œæ¼æ–—æŸ¥è¯¢
    conn = connect(host=host, port=21050)
    cur = conn.cursor()
    # quaryå¤„ç†
    from_month = "'" + quary[0] + "-" + quary[1] + "-01 00:00:00.000000000'"
    if int(quary[1]) < 12:
        to_month = "'" + quary[0] + "-" + "{:0>2d}".format(int(quary[1]) + 1) + "-01 00:00:00.000000000'"
    else:
        to_month = "'" + str(int(quary[0]) + 1) + "-01-01 00:00:00.000000000'"

    count0 = count1 = count2 = count3 = 0  # counté»˜è®¤ä¸º0

    # æŠ½å–åªå«æŸ¥è¯¢çŠ¶æ€çš„æ•°æ®

    # ä½¿ç”¨æŠ½æ ·æ•°æ®æ¼”ç¤º
    # random_sample(200)
    # create_string = "create view sample_funnel as select user_id, event_id, time from random_sample where event_id in" + \
    #                 str(event_ids) + " and " + from_month + " <time and time< " + to_month

    # æ€»è¡¨æµ‹è¯•
    create_string = "create view sample_funnel as select user_id, event_id, time from event_export_partition where event_id in" + \
                    str(event_ids) + " and " + from_month + " <time and time< " + to_month
    cur.execute('use group7')
    cur.execute('drop view if exists group7.sample_funnel')
    cur.execute(create_string)
    cur.execute('select count(time) from sample_funnel where event_id=' + str(event_ids[0]))
    count0 = cur.fetchall()[0][0]
    create_string = "select count(t1.time),count(t2.time), count(t3.time) from (select * from sample_funnel where event_id=" \
                    + str(event_ids[0]) + ") t0" + \
                    " left join (select * from sample_funnel where event_id=" + str(event_ids[1]) + ") t1" + \
                    " on t0.user_id=t1.user_id and t0.time<t1.time and timestamp_cmp(t0.time + interval 120 minutes, t1.time)=1" + \
                    " left join (select * from sample_funnel where event_id=" + str(event_ids[2]) + ") t2" + \
                    " on t1.user_id=t2.user_id and t1.time<t2.time and timestamp_cmp(t1.time + interval 120 minutes, t2.time)=1" + \
                    " left join (select * from sample_funnel where event_id=" + str(event_ids[3]) + ") t3" + \
                    " on t2.user_id=t3.user_id and t2.time<t3.time and timestamp_cmp(t2.time + interval 120 minutes, t3.time)=1"
    cur.execute(create_string)
    data = cur.fetchall()
    count1, count2, count3 = data[0][0], data[0][1], data[0][2]
    print([count0, count1, count2, count3])
    return [count0, count1, count2, count3]
```
#### 3. ç•™å­˜åˆ†æ

1. ç”¨æˆ·åœ¨é¡µé¢é€‰æ‹©

   * æ—¶é—´æ®µ( yyyy-mm-dd,yyyy-mmâ€”dd) 

   * åˆå§‹äº‹ä»¶ ï¼š event_id

   * åç»­äº‹ä»¶: event_id

2. æˆ‘ä»¬é¦–å…ˆè¦å°†å­—ç¬¦ä¸²çš„æ—¶é—´æ ¼å¼è½¬æ¢æˆUnixTimestamp

```python
    from_time += " 00:00:00"
    to_time += " 00:00:00"
    from_time = time.strptime(from_time, "%Y-%m-%d %H:%M:%S")
    from_day = str(int(time.mktime(from_time) // 86400))
    to_time = time.strptime(to_time, "%Y-%m-%d %H:%M:%S")
    to_day = str(int(time.mktime(to_time) // 86400))
```

3. åœ¨è¡¨ä¸­æŸ¥è¯¢æ‰€æœ‰åœ¨è§„å®šæ—¶é—´æ®µå†…è¿›è¡Œè¿‡åˆå§‹äº‹ä»¶çš„ç”¨æˆ·ï¼Œå¹¶ä¸ºä»–ä»¬åˆ›å»ºä¸€ä¸ªä¸´æ—¶è¡¨user_init_event

```python
                    "with user_init_event " \
                    "as (select user_id, day as init_day " \
                    "from event_export_partition_parquet_g7 " \
                    "where event_id = "+ event_init +" and day >= "+from_day+" and day <= "+to_day+" ),"
```

4. å°†äº‹ä»¶è¡¨å’Œuser_init_eventè¡¨æŒ‰ç…§user_id  joinï¼Œå¹¶ç­›é€‰å‡ºå…¶ä¸­äº‹ä»¶ä¸ºåç»­äº‹ä»¶å¹¶ä¸”åç»­äº‹ä»¶å’Œåˆå§‹äº‹ä»¶çš„æ—¶é—´é—´éš”åœ¨0-7å¤©ï¼ŒæŠŠè¿™äº›ç”¨æˆ·çš„id,å‘ç”Ÿåˆå§‹äº‹ä»¶çš„æ—¶é—´ï¼Œæ—¶é—´é—´éš” å­˜åˆ°ä¸´æ—¶è¡¨ user_cohort ä¸­ã€‚

```python
"user_cohort as( " \
                    "select e.user_id,i.init_day,(e.day-i.init_day) as cohort_day " \
                    "from event_export_partition_parquet_g7 e LEFT JOIN user_init_event i on e.user_id = i.user_id " \
                    "where e.event_id = "+ event_remain+ " and (e.day-i.init_day)<7 and (e.day-i.init_day)>=0 " 
      							"group by user_id,cohort_day,i.init_day)" \
```

5. åœ¨user_cohortè¡¨ä¸­ ï¼ŒæŒ‰ç…§åˆå§‹äº‹ä»¶çš„æ—¶é—´ å’Œ ç•™å­˜æ—¶é—´åˆ†ç»„ å¹¶ä»¥åˆå§‹æ—¶é—´å’Œç•™å­˜æ—¶é—´æ’åºï¼Œè®¡ç®—æ¯ç»„ä¸­çš„äººæ•°ã€‚

```python
"select count(*),cohort_day,init_day from user_cohort group by init_day,cohort_day order by init_day,cohort_day"
```

6. æ€»çš„å‡½æ•°

```sql
def remain2(from_time,to_time,event_init,event_remain):
    from_time += " 00:00:00"
    to_time += " 00:00:00"
    from_time = time.strptime(from_time, "%Y-%m-%d %H:%M:%S")
    from_day = str(int(time.mktime(from_time) // 86400))
    to_time = time.strptime(to_time, "%Y-%m-%d %H:%M:%S")
    to_day = str(int(time.mktime(to_time) // 86400))

    cur.execute("use rawdata")
    create_string = "with user_init_event " \
                    "as (select user_id, day as init_day " \
                    "from event_export_partition_parquet_g7 " \
                    "where event_id = "+ event_init +" and day >= "+from_day+" and day <= "+to_day+" )," \
                    "user_cohort as( " \
                    "select e.user_id,i.init_day,(e.day-i.init_day) as cohort_day " \
                    "from event_export_partition_parquet_g7 e LEFT JOIN user_init_event i on e.user_id = i.user_id " \
                    "where e.event_id = "+ event_remain+ " and (e.day-i.init_day)<7 and (e.day-i.init_day)>=0 " \
                    "group by user_id,cohort_day,i.init_day)" \
                    "select count(*),cohort_day,init_day from user_cohort group by init_day,cohort_day order by init_day,cohort_day"

    start = datetime.datetime.now()
    cur.execute(create_string)
    res = cur.fetchall()
    end = datetime.datetime.now()

    print(res)
    print(end - start)
```

#### 4. webåç«¯

   ä¸ºäº†æ–¹ä¾¿å±•ç¤ºï¼Œæˆ‘ä»¬é‡‡ç”¨webé¡µé¢çš„æ–¹å¼å‘ç”¨æˆ·æä¾›æœåŠ¡ã€‚ç”¨æˆ·å¯ä»¥åœ¨ç½‘é¡µä¸Šè¿›è¡Œè®¾ç½®ä»¥é€‰æ‹©è‡ªå·±éœ€è¦çš„æœåŠ¡å½¢å¼ã€‚

   å…·ä½“å®ç°æ–¹æ³•ä¸ºåŸºäºDjangoæ¨¡æ¿å¼•æ“çš„Pythonæ–¹æ³•ã€‚æˆ‘ä»¬ä¸ºç”¨æˆ·åˆ›å»ºfunnel,event,remainä¸‰ä¸ªé¡µé¢ã€‚åˆ†åˆ«å¯¹åº”æ¼æ–—åˆ†æï¼Œäº‹ä»¶åˆ†æï¼Œç•™å­˜åˆ†æã€‚ç”¨æˆ·åœ¨åœ°å€æ è¾“å…¥ç›¸åº”URLï¼Œç”¨æˆ·è¾“å…¥ä½œä¸ºPOSTæŠ¥æ–‡å†…å®¹ä¼ è‡³åç«¯ï¼Œåç«¯æ ¹æ®urlå°†è·¯ç”±åˆ†å‘åˆ°ç›¸åº”çš„å¤„ç†æ¨¡å—ã€‚å¤„ç†æ¨¡å—å¤„ç†ç”¨æˆ·POSTæŠ¥æ–‡ä¸­çš„å‚æ•°ä¿¡æ¯ã€‚å¹¶å°†è¿™äº›ä¿¡æ¯ä½œä¸ºå‚æ•°è°ƒç”¨ç›¸åº”çš„æŸ¥è¯¢æ–¹æ³•å‘é€åˆ°impalaæœåŠ¡å™¨ä»¥è·å¾—æ­£ç¡®çš„æŸ¥è¯¢ç»“æœã€‚

   ![å±å¹•å¿«ç…§ 2019-07-18 ä¸Šåˆ9.34.28](http://ww2.sinaimg.cn/large/006tNc79ly1g53qpm3l2gj30zs0kt4qp.jpg)

#### 5. å¯è§†åŒ–

   æ¼æ–—å›¾å’Œäº‹ä»¶åˆ†æçš„å›¾è¡¨é€šè¿‡pyechartsç»˜åˆ¶ã€‚è°ƒç”¨pyechartsåŒ…é‡Œçš„Lineç»˜åˆ¶æŠ˜çº¿å›¾ï¼ŒFunnelç»˜åˆ¶æ¼æ–—å›¾ï¼Œ

   ![funnel](http://ww2.sinaimg.cn/large/006tNc79ly1g53qsvhlpzj31980lgdhf.jpg)

   ![event](http://ww4.sinaimg.cn/large/006tNc79ly1g53qthixy0j31880mjdii.jpg)

## ä¼˜åŒ–æ–¹æ³•

1. å­˜å‚¨æ–¹å¼

   å°†TEXTæ•°æ®è½¬æ¢æˆParquetå­˜å‚¨

2. åˆ†åŒº

   å°†æ•°æ®æŒ‰ç…§ (day, event_bucket )åˆ†åŒº

> create table rawdata.parquet_partiton(
>
> xxx
>
>  ) 
>
> select ( xxxx ,day, event_bucket) from xxx
>
>  stored as parquet 
>
> partitoned by(day,event_bucket)

3. åœ¨SQLè¯­å¥ä¸­ï¼Œæ—¶é—´ç»´åº¦ä¸Šçš„ç­›é€‰æˆ‘ä»¬å°½é‡åœ¨ç”¨day æ¥ä½œä¸ºæŸ¥è¯¢æ¡ä»¶ï¼Œä»¥æé«˜æŸ¥è¯¢æ•ˆç‡ã€‚

4. åˆ†æSQLè¯­å¥æ€§èƒ½ï¼Œå°½é‡é™ä½å‡ºç°`select *`ï¼ŒåŒæ—¶é™ä½SQLçš„æ—¶é—´å¤æ‚åº¦

5. åœ¨è¿›è¡ŒæŸ¥è¯¢ä¹‹å‰ï¼Œä¸ºäº†é¿å…åœ¨æ€»è¡¨ä¸­è¿›è¡ŒæŸ¥è¯¢ï¼Œç”±äºæ•°æ®é‡é—®é¢˜æ‹–æ…¢æŸ¥è¯¢é€Ÿåº¦ï¼Œæˆ‘ä»¬å¯¹æŸ¥è¯¢æ•°æ®åšäº†éƒ¨åˆ†ä¸å¤„ç†ï¼Œç­›é€‰å‡ºç”¨æˆ·é€‰æ‹©èŒƒå›´å†…çš„æ—¶é—´ä¸äº‹ä»¶ï¼Œä½¿åç»­çš„æŸ¥è¯¢æ›´é«˜æ•ˆï¼Œç”¨æˆ·å“åº”æ—¶é—´æ›´çŸ­ã€‚

## æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ

ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®éœ€å¯¹é¡¹ç›®ä¸­ä¸‰ä¸ªåŠŸèƒ½å‡½æ•°è¿›è¡Œæµ‹è¯•ã€‚é’ˆå¯¹ä¸åŒçš„åŠŸèƒ½éœ€è¦çš„æ•°æ®ç»´åº¦ä¸åŒï¼Œå…·ä½“å¦‚ä¸‹ï¼š

day,time,user_id,event_id, p_utm_source,event_bucket

event_id: 26,8,18,22,27->å®Œæˆé¡¹ç›®åˆ›å»º
event_bucket:0-19

### 1. æ¼æ–—åˆ†æ

æ¼æ–—åˆ†æä¸­éœ€è¦çš„æ•°æ®å±æ€§åŒ…æ‹¬ï¼štime, event_id, day, user_id

æ ¹æ®æ¼æ–—åˆ†æçš„ç‰¹ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰å¥—æµ‹è¯•æµç¨‹ï¼Œç”Ÿæˆæ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œå…·ä½“å¦‚ä¸‹ ï¼š

1. ç‚¹å‡»æ³¨å†Œ event_id=26 
2. æ³¨å†Œ-è·å–éªŒè¯ç  event_id=8
3. æ³¨å†Œ-è¾“å…¥éªŒè¯ç  event_id=18
4. å®Œæˆæ³¨å†Œ event_id=22
> å¥½åƒåªå†™å®Œäº†ğŸ‘†ğŸ¿ğŸ‘†è¿™ä¸ªç¬¬ä¸€å¥—æµç¨‹ï¼Œå®³æ²¡å…³ç³»å‰©ä¸‹çš„éƒ½ä¸€æ ·ï¼å—¯ï¼

1. ç‚¹å‡»å¿˜è®°å¯†ç  event_id=5
2. æ‰¾å›å¯†ç -è·å–éªŒè¯ç  event_id=19
3. æ‰¾å›å¯†ç -é‡ç½®å¯†ç  event_id=28
4. æäº¤æ–°å¯†ç  event_id=1

1. åˆ›å»ºé¡¹ç›®-é€‰æ‹©é¡¹ç›®æ¨¡ç‰ˆ event_id=16
2. åˆ›å»ºé¡¹ç›®-æ·»åŠ å›¢é˜Ÿæˆå‘˜ event_id=12
3. åˆ›å»ºé¡¹ç›®-æ·»åŠ å®¢æˆ· event_id=15
4. å®Œæˆé¡¹ç›®åˆ›å»º event_id=27

### 2. äº‹ä»¶åˆ†æ

äº‹ä»¶åˆ†æä¸­éœ€è¦çš„æ•°æ®å±æ€§åŒ…æ‹¬ï¼štime, event_id, event_feature, day, user_id

å¯ä»¥æŸ¥å®Œæˆæ³¨å†Œçš„äººï¼Œå¯ä»¥ç›´æ¥ç”¨æ¼æ–—åˆ†æçš„ç”Ÿæˆæ•°æ®è¿›è¡Œæµ‹è¯•



### 3. ç•™å­˜åˆ†æ

ç•™å­˜åˆ†æä¸­éœ€è¦çš„æ•°æ®å±æ€§åŒ…æ‹¬ï¼štime, day, event_id, user_id

æ ¹æ®ç•™å­˜åˆ†æçš„ç‰¹ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸‹æµ‹è¯•æµç¨‹ï¼Œç”Ÿæˆæ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

1. åˆå§‹è¡Œä¸ºï¼šç‚¹å‡»æ³¨å†Œ event_id=26
2. åç»­è¡Œä¸ºï¼šå®Œæˆæ³¨å†Œ event_id=22



## æ­£ç¡®æ€§æµ‹è¯•

æ ¹æ®æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆæ—¶è®¾å®šçš„è§„åˆ™ï¼ŒåŠè®¡ç®—å¥½çš„æ¯ä¸ªæ­¥éª¤çš„æ¯”ä¾‹ä¸é¢„æœŸè¾“å‡ºç»“æœï¼Œå°†å…¶ä½œä¸ºæˆ‘ä»¬æŸ¥è¯¢ç»“æœæ­£ç¡®æ€§çš„æ ‡å‡†ï¼Œä½¿ç”¨æˆ‘ä»¬ç”Ÿæˆå¥½çš„1äº¿æ¡æ•°æ®å¯¹å‡½æ•°è¿›è¡Œæµ‹è¯•ã€‚

æµ‹è¯•ç»“æœå…·ä½“å¦‚ä¸‹ï¼š

1. æ¼æ–—åˆ†æ
  
  * 1->2æŸ¥è¯¢ç»“æœ96%çš„ç”¨æˆ·å®Œæˆç¬¬äºŒæ­¥è¡Œä¸º

  * 2->3æŸ¥è¯¢ç»“æœ87.5%çš„ç”¨æˆ·å®Œæˆç¬¬ä¸‰æ­¥è¡Œä¸º

  * 3->4æŸ¥è¯¢ç»“æœä¸º85.7%çš„ç”¨æˆ·å®Œæˆç¬¬å››æ­¥è¡Œä¸º

ç”±äºç”Ÿæˆæ•°æ®å¹¶ä¸ç¬¦åˆå®é™…å•†ä¸šç¯å¢ƒä¸­çš„è§„å¾‹ï¼Œå› æ­¤è¿™é‡Œå°±ä¸è´´å‡ºæ›´å¤šçš„è¿è¡Œç»“æœã€‚

2. äº‹ä»¶åˆ†æ

  * æŸ¥è¯¢æ³¨å†Œçš„äººæ•° ç»“æœç¬¦åˆä¸Šè¿°ç”Ÿæˆæ•°æ®ç»“æœ

3. ç•™å­˜åˆ†æ

  * æŸ¥è¯¢ç‚¹å‡»æ³¨å†Œåˆ°å®Œæˆé¡¹ç›®åˆ›å»ºçš„äººæ•°ï¼Œç»“æœç¬¦åˆä¸Šè¿°æ•°æ®ç”Ÿæˆé¢„æœŸã€‚

## æ€§èƒ½æµ‹è¯•

1. æ¼æ–—åˆ†æ 0.83s

æ ¹æ®ç½‘é¡µä¼ å…¥åå°çš„æ•°æ®ï¼ŒåŠä¸Šè¿°funnelå‡½æ•°çš„è®¾è®¡ï¼ŒæŸ¥è¯¢è¿”å›çš„count0,count1,count2,count3å³åˆ†åˆ«é—®å®Œæˆç¬¬ä¸€ã€äºŒã€ä¸‰ã€å››æ­¥éª¤çš„ç”¨æˆ·æ•°ã€‚

```
Query progress can be monitored at: http://lesson7:25000/query_plan?query_id=7b4c45698088923d:e8024a3000000000
+-------------+
| count(time) |
+-------------+
| 13010       |
+-------------+
Fetched 1 row(s) in 0.13s
Query progress can be monitored at: http://lesson7:25000/query_plan?query_id=9b49aa72dee929b7:e48836f700000000
+----------------+----------------+----------------+
| count(t1.time) | count(t2.time) | count(t3.time) |
+----------------+----------------+----------------+
| 17520          | 152            | 64             |
+----------------+----------------+----------------+
Fetched 1 row(s) in 0.70s
```
æŸ¥è¯¢æ•°æ®é‡450w++

2. äº‹ä»¶åˆ†æ 0.37s

æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„åˆ†æfeaturesï¼ŒæŸ¥è¯¢è¿”å›ç”¨æˆ·æŒ‡å®šå½¢å¼çš„æ‰§è¡Œäº‹ä»¶åˆ†ææŒ‡æ ‡ï¼Œè¿™é‡Œä»¥æ‰§è¡Œæ¬¡æ•°ä¸ºä¾‹ï¼š
```
Query progress can be monitored at: http://lesson7:25000/query_plan?query_id=66465fa641f10258:b983975b00000000
+-------------+-------+
| count(time) | day   |
+-------------+-------+
| 456         | 17898 |
| 360         | 17899 |
| 380         | 17900 |
| 414         | 17901 |
| 478         | 17902 |
| 416         | 17903 |
| ...         | ...   |
| 548         | 17919 |
| 546         | 17920 |
| 508         | 17921 |
| 518         | 17922 |
| 558         | 17923 |
| 512         | 17924 |
| 512         | 17925 |
| 544         | 17926 |
| 506         | 17927 |
+-------------+-------+
Fetched 30 row(s) in 0.14s
```
åæ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ä¸åŒgroupå› ç´ ï¼Œè¿›è¡Œåˆ†ç»„å±•ç¤ºæŸ¥è¯¢ç»“æœï¼Œè¿™é‡Œä»¥p__carrierä¸ºä¾‹ï¼š
```
Query progress can be monitored at: http://lesson7:25000/query_plan?query_id=f24e01f460b00685:940684a200000000
+-------------+------------+-------+
| count(time) | p__carrier | day   |
+-------------+------------+-------+
| 164         | ä¸­å›½ç”µä¿¡   | 17898 |
| 132         | ä¸­å›½ç§»åŠ¨   | 17898 |
| 160         | ä¸­å›½è”é€š   | 17898 |
| 120         | ä¸­å›½è”é€š   | 17899 |
| 134         | ä¸­å›½ç§»åŠ¨   | 17899 |
| ...         | ...       | ...   |
| 166         | ä¸­å›½è”é€š   | 17907 |
| 182         | ä¸­å›½ç§»åŠ¨   | 17923 |
| 178         | ä¸­å›½ç”µä¿¡   | 17924 |
| 138         | ä¸­å›½ç§»åŠ¨   | 17924 |
| 196         | ä¸­å›½è”é€š   | 17924 |
| 162         | ä¸­å›½ç”µä¿¡   | 17925 |
| 166         | ä¸­å›½ç§»åŠ¨   | 17925 |
| 184         | ä¸­å›½è”é€š   | 17925 |
| 152         | ä¸­å›½è”é€š   | 17926 |
| 186         | ä¸­å›½ç”µä¿¡   | 17926 |
| 206         | ä¸­å›½ç§»åŠ¨   | 17926 |
| 184         | ä¸­å›½ç”µä¿¡   | 17927 |
| 150         | ä¸­å›½è”é€š   | 17927 |
| 172         | ä¸­å›½ç§»åŠ¨   | 17927 |
+-------------+------------+-------+
Fetched 90 row(s) in 0.23s
```
æŸ¥è¯¢æ•°æ®é‡450w++

3. ç•™å­˜åˆ†æ 0.39s

æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ä¸åŒèµ·å§‹å’Œç»“æŸæ—¶é—´ï¼Œæˆ‘ä»¬è¿”å›èµ·å§‹äº‹ä»¶ä¹‹åçš„0ï½7æ—¥ç•™å­˜ï¼š

```
+----------+------------+----------+
| count(*) | cohort_day | init_day |
+----------+------------+----------+
| 123      | 0          | 17897    |
| 2        | 1          | 17897    |
| 4        | 5          | 17897    |
| 2        | 6          | 17897    |
| 153      | 0          | 17898    |
| 1        | 1          | 17898    |
| 1        | 2          | 17898    |
| 3        | 3          | 17898    |
| 3        | 5          | 17898    |
| ...      | ...        | ...      |
| 1        | 2          | 17925    |
| 2        | 3          | 17925    |
| 2        | 4          | 17925    |
| 3        | 5          | 17925    |
| 181      | 0          | 17926    |
| 2        | 1          | 17926    |
| 3        | 2          | 17926    |
| 2        | 3          | 17926    |
| 1        | 4          | 17926    |
| 2        | 5          | 17926    |
| 6        | 6          | 17926    |
+----------+------------+----------+
Fetched 179 row(s) in 0.37s
```
æŸ¥è¯¢æ•°æ®é‡450w++

é¡¹ç›®åœ°å€åŠå®Œæ•´æºç ï¼š

https://github.com/41xu/user-behavior-analysis

https://github.com/bellick/user-behavior-analysis
